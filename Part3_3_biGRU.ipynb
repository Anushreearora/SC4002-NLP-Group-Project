{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch gensim datasets nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8530 sentences\n",
      "Size of validation set: 1066 sentences\n",
      "Size of test set: 1066 sentences\n"
     ]
    }
   ],
   "source": [
    "#Number of sentences in each set \n",
    "print(f\"Size of training set: {train_dataset.num_rows} sentences\")\n",
    "print(f\"Size of validation set: {validation_dataset.num_rows} sentences\")\n",
    "print(f\"Size of test set: {test_dataset.num_rows} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding matrix with shape: (16633, 300)\n",
      "Vocabulary size (word_to_index): 16633\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the embedding matrix and word_to_index from the pickle file\n",
    "with open(\"updated_embedding_matrix.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    embedding_matrix = data[\"embeddings\"]\n",
    "    word_to_index = data[\"word_to_index\"]\n",
    "\n",
    "# Convert embedding_matrix to a NumPy array and a PyTorch tensor\n",
    "embedding_matrix_array = np.array(embedding_matrix)\n",
    "embedding_matrix_tensor = torch.tensor(embedding_matrix_array, dtype=torch.float32)\n",
    "\n",
    "print(f\"Loaded embedding matrix with shape: {embedding_matrix_array.shape}\")\n",
    "print(f\"Vocabulary size (word_to_index): {len(word_to_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenise train, validation, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenized_train_texts = []\n",
    "for sentence in train_dataset['text']:\n",
    "    # Tokenize the sentence using spaCy and store tokens as a list of strings\n",
    "    tokens = [token.text for token in nlp(sentence.lower())]\n",
    "    pre_tokenized_train_texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenize validation and test sets\n",
    "pre_tokenized_validation_texts = [[token.text for token in nlp(sentence.lower())] for sentence in validation_dataset['text']]\n",
    "pre_tokenized_test_texts = [[token.text for token in nlp(sentence.lower())] for sentence in test_dataset['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence: ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', '``', 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'] \n",
      "\n",
      "Number of words in the vocabulary(including padding and unknown tokens): 18031\n",
      "Number of words in the vocabulary: 18029\n"
     ]
    }
   ],
   "source": [
    "# #tokenize sentences \n",
    "# train_tokenized = []\n",
    "# for sentence in train_dataset['text']:\n",
    "#     train_tokenized.append(word_tokenize(sentence.lower()))\n",
    "\n",
    "# print('sample sentence:', train_tokenized[0],'\\n')\n",
    "\n",
    "# #build vocabulary\n",
    "# vocab = {\"<PAD>\", \"<UNK>\"} #include a padding and unknown token for future processing\n",
    "# vocab.update(word for sentence in train_tokenized for word in sentence)\n",
    "\n",
    "# print(\"Number of words in the vocabulary(including padding and unknown tokens):\", len(vocab))\n",
    "# print(\"Number of words in the vocabulary:\" , len(vocab)-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 \n",
    "# 3.Keeping the above two adjustments, replace your simple RNN model in Part 2 wioth a biLSTM model and biGRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import numpy as np\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix=np.load(\"embedding_matrix.npy\",allow_pickle='TRUE').item()\n",
    "# # display(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures reproducibility in CUDA operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disables some optimizations to ensure determinism\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels : list[int], vocab : set, embedding_matrix : dict, max_len=30):\n",
    "        self.texts = tokenized_texts\n",
    "        self.labels  = labels\n",
    "        self.vocab = word_to_index\n",
    "        self.embedding_matrix : dict = embedding_matrix\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        vectorized_text = self.vectorize(text)\n",
    "        return torch.tensor(vectorized_text, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "            \n",
    "    def vectorize(self, tokens):\n",
    "        # Convert tokens to their corresponding index in the vocabulary\n",
    "        vectorized = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "        # Pad or truncate to max_len\n",
    "        if len(vectorized) < self.max_len:\n",
    "            vectorized += [self.vocab['<PAD>']] * (self.max_len - len(vectorized))\n",
    "        else:\n",
    "            vectorized = vectorized[:self.max_len]\n",
    "        return vectorized\n",
    "\n",
    "    def build_vocab_dict(self, vocab : set):\n",
    "        if \"<PAD>\" in vocab : vocab.remove(\"<PAD>\")\n",
    "        if \"<UNK>\" in vocab : vocab.remove(\"<UNK>\")\n",
    "        vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "        vocab_dict['<PAD>'] = len(vocab_dict) # Add padding token\n",
    "        vocab_dict['<UNK>'] = len(vocab_dict) # Add unknown token\n",
    "        return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting the Vocab set to dictionary \n",
    "# def build_vocab_dict(vocab_set):\n",
    "#     # Create the vocabulary dictionary without <PAD> and <UNK>\n",
    "#     vocab_set.discard(\"<PAD>\")\n",
    "#     vocab_set.discard(\"<UNK>\")\n",
    "#     vocab_dict = {word: idx for idx, word in enumerate(vocab_set, start=2)}\n",
    "\n",
    "#     # Check for <PAD> and <UNK> existence and assign them fixed indices if they are present\n",
    "#     if \"<PAD>\" not in vocab_dict:\n",
    "#         vocab_dict[\"<PAD>\"] = 0  # Index for padding token\n",
    "#     if \"<UNK>\" not in vocab_dict:\n",
    "#         vocab_dict[\"<UNK>\"] = 1  # Index for unknown token\n",
    "    \n",
    "#     #add the <PAD> and <UNK> back to the vocab\n",
    "#     vocab_set.add(\"<PAD>\")\n",
    "#     vocab_set.add(\"<UNK>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# biGRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentBiGRU(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SentimentBiGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Bidirectional GRU layer\n",
    "        self.gru = nn.GRU(embedding_matrix.size(1), hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Output size is 2 * hidden_dim for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer for regularization\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Shape: [batch_size, sequence_length, embedding_dim]\n",
    "        gru_out, _ = self.gru(embedded)  # gru_out shape: [batch_size, sequence_length, hidden_dim * 2]\n",
    "        # Concatenate last hidden states from forward and backward GRUs\n",
    "        out = torch.cat((gru_out[:, -1, :self.hidden_dim], gru_out[:, 0, self.hidden_dim:]), dim=1)  # Shape: [batch_size, hidden_dim * 2]\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "        return self.fc(out)  # Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding_matrix : dict[ str , np.ndarray]= np.load(\"embedding_matrix.npy\",allow_pickle='TRUE').item()\n",
    "# embedding_matrix_values = np.array(list(embedding_matrix.values()), dtype=np.float32)\n",
    "# embedding_matrix_tensor = torch.tensor(embedding_matrix_values, dtype=torch.float32)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = SentimentDataset(pre_tokenized_train_texts, train_dataset['label'], word_to_index, embedding_matrix)\n",
    "valid_dataset = SentimentDataset(pre_tokenized_validation_texts, validation_dataset['label'], word_to_index, embedding_matrix)\n",
    "test_dataset = SentimentDataset(pre_tokenized_test_texts, test_dataset['label'], word_to_index, embedding_matrix)\n",
    "# Create data loaders\n",
    "train_iterator = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in iterator:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch).squeeze(1)\n",
    "        loss = criterion(output, y_batch.float())\n",
    "        loss.backward()\n",
    "        # for param in model.parameters():\n",
    "        #     if param.grad is not None:  # Ensure the gradient is not None\n",
    "        #         print(f\"Gradient norm for {param.shape}: {param.grad.data.norm()}\")\n",
    "        optimizer.step()\n",
    "        # print(\"model.parameters():\",model.parameters())\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in iterator:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "            output = model(X_batch).squeeze(1)\n",
    "            \n",
    "            probs = model.sigmoid(output)\n",
    "            prediction = (probs >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(output, y_batch.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(prediction.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy, epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(num_epochs, model, train_iterator, valid_iterator, optimizer, criterion, scheduler):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "        accuracy , valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        if (epoch==0):\n",
    "            best_acc = accuracy\n",
    "            epochs_without_improvement = 0\n",
    "        print(f'Epoch {epoch + 1}: Train Loss = {train_loss:.3f}, Accuracy = {accuracy:.3f}, Val Loss = {valid_loss:.3f} Learning Rate: {scheduler.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Check for convergence\n",
    "        if epochs_without_improvement >= 6:  # Convergence condition (no improvement for 4 epochs)\n",
    "            print(\"Convergence reached, stopping training.\")\n",
    "            break\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128  # Adjust as needed\n",
    "output_dim = 1  # Binary sentiment classification\n",
    "dropout_rate = 0.3  # Adjust as needed\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = SentimentBiGRU(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate=0.5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.548, Accuracy = 0.780, Val Loss = 0.470 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.323, Accuracy = 0.780, Val Loss = 0.492 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.150, Accuracy = 0.765, Val Loss = 0.562 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.057, Accuracy = 0.755, Val Loss = 0.857 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.025, Accuracy = 0.750, Val Loss = 0.892 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.009, Accuracy = 0.741, Val Loss = 1.338 Learning Rate: 0.001000\n",
      "Convergence reached, stopping training.\n"
     ]
    }
   ],
   "source": [
    "# Now you can run your training loop\n",
    "train_and_validate(30, model, train_iterator, valid_iterator, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.3, batch_size=32\n",
      "Epoch 1: Train Loss = 0.563, Accuracy = 0.747, Val Loss = 0.495 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.330, Accuracy = 0.765, Val Loss = 0.489 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.156, Accuracy = 0.765, Val Loss = 0.644 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.066, Accuracy = 0.757, Val Loss = 0.770 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.026, Accuracy = 0.757, Val Loss = 0.970 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.011, Accuracy = 0.754, Val Loss = 1.103 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.008, Accuracy = 0.756, Val Loss = 1.243 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.003, Accuracy = 0.752, Val Loss = 1.260 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.002, Accuracy = 0.750, Val Loss = 1.288 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.3, batch_size=64\n",
      "Epoch 1: Train Loss = 0.595, Accuracy = 0.769, Val Loss = 0.487 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.373, Accuracy = 0.767, Val Loss = 0.489 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.208, Accuracy = 0.773, Val Loss = 0.572 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.094, Accuracy = 0.757, Val Loss = 0.748 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.051, Accuracy = 0.758, Val Loss = 0.938 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.023, Accuracy = 0.757, Val Loss = 1.008 Learning Rate: 0.000100\n",
      "Epoch 7: Train Loss = 0.010, Accuracy = 0.760, Val Loss = 1.072 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.008, Accuracy = 0.757, Val Loss = 1.104 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.007, Accuracy = 0.757, Val Loss = 1.144 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.5, batch_size=32\n",
      "Epoch 1: Train Loss = 0.563, Accuracy = 0.776, Val Loss = 0.472 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.335, Accuracy = 0.774, Val Loss = 0.508 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.158, Accuracy = 0.782, Val Loss = 0.568 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.068, Accuracy = 0.761, Val Loss = 0.780 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.030, Accuracy = 0.747, Val Loss = 1.043 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.017, Accuracy = 0.756, Val Loss = 1.051 Learning Rate: 0.000100\n",
      "Epoch 7: Train Loss = 0.007, Accuracy = 0.754, Val Loss = 1.132 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.005, Accuracy = 0.755, Val Loss = 1.179 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.005, Accuracy = 0.755, Val Loss = 1.206 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.5, batch_size=64\n",
      "Epoch 1: Train Loss = 0.594, Accuracy = 0.758, Val Loss = 0.507 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.377, Accuracy = 0.765, Val Loss = 0.487 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.206, Accuracy = 0.765, Val Loss = 0.550 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.098, Accuracy = 0.757, Val Loss = 0.763 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.049, Accuracy = 0.745, Val Loss = 0.953 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.025, Accuracy = 0.748, Val Loss = 1.046 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.013, Accuracy = 0.744, Val Loss = 1.261 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.006, Accuracy = 0.747, Val Loss = 1.272 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.005, Accuracy = 0.749, Val Loss = 1.288 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.3, batch_size=32\n",
      "Epoch 1: Train Loss = 0.535, Accuracy = 0.775, Val Loss = 0.467 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.314, Accuracy = 0.780, Val Loss = 0.489 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.139, Accuracy = 0.776, Val Loss = 0.587 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.050, Accuracy = 0.763, Val Loss = 0.806 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.021, Accuracy = 0.762, Val Loss = 0.943 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.009, Accuracy = 0.776, Val Loss = 1.121 Learning Rate: 0.000100\n",
      "Epoch 7: Train Loss = 0.003, Accuracy = 0.764, Val Loss = 1.147 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.002, Accuracy = 0.765, Val Loss = 1.191 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.3, batch_size=64\n",
      "Epoch 1: Train Loss = 0.569, Accuracy = 0.759, Val Loss = 0.518 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.354, Accuracy = 0.773, Val Loss = 0.476 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.197, Accuracy = 0.764, Val Loss = 0.563 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.098, Accuracy = 0.762, Val Loss = 0.694 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.037, Accuracy = 0.769, Val Loss = 0.971 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.016, Accuracy = 0.764, Val Loss = 1.131 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.009, Accuracy = 0.762, Val Loss = 1.065 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.004, Accuracy = 0.767, Val Loss = 1.168 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.5, batch_size=32\n",
      "Epoch 1: Train Loss = 0.550, Accuracy = 0.776, Val Loss = 0.469 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.332, Accuracy = 0.776, Val Loss = 0.483 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.152, Accuracy = 0.760, Val Loss = 0.630 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.062, Accuracy = 0.756, Val Loss = 0.806 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.022, Accuracy = 0.766, Val Loss = 1.032 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.008, Accuracy = 0.755, Val Loss = 1.094 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.5, batch_size=64\n",
      "Epoch 1: Train Loss = 0.574, Accuracy = 0.745, Val Loss = 0.493 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.364, Accuracy = 0.768, Val Loss = 0.468 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.206, Accuracy = 0.754, Val Loss = 0.562 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.097, Accuracy = 0.766, Val Loss = 0.743 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.044, Accuracy = 0.762, Val Loss = 0.917 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.023, Accuracy = 0.751, Val Loss = 0.970 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.010, Accuracy = 0.762, Val Loss = 1.229 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.003, Accuracy = 0.758, Val Loss = 1.309 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Best accuracy: 0.767 with hidden_dim=128, dropout_rate=0.3, batch_size=64\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the hyper-parameter grid\n",
    "hidden_dims = [64, 128]\n",
    "learning_rates = [0.001]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "batch_sizes = [32, 64]\n",
    "output_dim = 1 \n",
    "best_acc = 0\n",
    "best_hidden_dim = 0\n",
    "best_dropout_rate = 0\n",
    "best_bs = 0\n",
    "\n",
    "# Iterate over all combinations of hyper-parameters\n",
    "for hidden_dim, lr, dropout_rate, bs in itertools.product(hidden_dims, learning_rates, dropout_rates, batch_sizes):\n",
    "    print(f'Training with hidden_dim={hidden_dim}, lr={lr}, dropout_rate={dropout_rate}, batch_size={bs}')\n",
    "    \n",
    "    model_hyper = SentimentBiGRU(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "    criterion_hyper = nn.BCEWithLogitsLoss()\n",
    "    optimizer_hyper = optim.Adam(model_hyper.parameters(), lr=lr)\n",
    "    scheduler_hyper = optim.lr_scheduler.ReduceLROnPlateau(optimizer_hyper, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "    train_iterator_hyper = DataLoader(train_dataset, bs, shuffle=True)\n",
    "    valid_iterator_hyper = DataLoader(valid_dataset, bs, shuffle=False)\n",
    "\n",
    "    accuracy = train_and_validate(25, model_hyper, train_iterator_hyper, valid_iterator_hyper, optimizer_hyper, criterion_hyper, scheduler_hyper)\n",
    "\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_hidden_dim = hidden_dim\n",
    "        best_dropout_rate = dropout_rate\n",
    "        best_bs = bs\n",
    "\n",
    "\n",
    "print(f'Best accuracy: {best_acc:.3f} with hidden_dim={best_hidden_dim}, dropout_rate={best_dropout_rate}, batch_size={best_bs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with best parameters \n",
    "hidden_dim=64\n",
    "dropout_rate=0.3\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.283, Accuracy = 0.500, Val Loss = 2.815 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.446, Accuracy = 0.500, Val Loss = 2.534 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.426, Accuracy = 0.500, Val Loss = 2.457 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.424, Accuracy = 0.500, Val Loss = 2.444 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.422, Accuracy = 0.500, Val Loss = 2.255 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.354, Accuracy = 0.500, Val Loss = 2.639 Learning Rate: 0.001000\n",
      "Convergence reached, stopping training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim_best = 128\n",
    "lr=0.001\n",
    "dropout_rate_best=0.3\n",
    "bs_best=32\n",
    "output_dim = 1 \n",
    "\n",
    "model_best = SentimentBiGRU(embedding_matrix_tensor, hidden_dim_best, output_dim, dropout_rate_best).to(device)\n",
    "criterion_best = nn.BCEWithLogitsLoss()\n",
    "optimizer_best = optim.Adam(model_best.parameters(), lr)\n",
    "scheduler_best = optim.lr_scheduler.ReduceLROnPlateau(optimizer_best, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "train_iterator_best = DataLoader(train_dataset, bs_best)\n",
    "valid_iterator_best = DataLoader(valid_dataset, bs_best)\n",
    "\n",
    "train_and_validate(25, model_best, train_iterator_best, valid_iterator_best, optimizer_best, criterion_best, scheduler_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in iterator:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "            output = model(X_batch).squeeze(1)\n",
    "            \n",
    "            probs = model.sigmoid(output)\n",
    "            prediction = (probs >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(output, y_batch.float())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(prediction.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "            \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.775\n"
     ]
    }
   ],
   "source": [
    "test(model_best, test_iterator, criterion_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiGRU with an Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: [batch_size, hidden_dim] (last hidden state of the decoder or a step in the sequence)\n",
    "        encoder_outputs: [batch_size, seq_len, hidden_dim] (outputs from the encoder)\n",
    "        \"\"\"\n",
    "        # Compute the attention weights using the query (hidden state) and keys (encoder outputs)\n",
    "        attn_weights = torch.matmul(encoder_outputs, hidden.unsqueeze(2)).squeeze(2) # [batch_size, seq_len]\n",
    "        \n",
    "        # Apply softmax to get the attention weights\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        # Compute the weighted sum of the encoder outputs (values)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1) # [batch_size, hidden_dim]\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiGRUWithAttention(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout_rate):\n",
    "        super(BiGRUWithAttention, self).__init__()\n",
    "        # Load pre-trained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        \n",
    "        # Define the GRU layer (bidirectional)\n",
    "        self.gru = nn.GRU(embedding_matrix.size(1), hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attn = Attention(hidden_dim * 2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x) # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get the outputs and hidden states from the GRU\n",
    "        gru_out, _ = self.gru(embedded) # gru_out: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Use the last hidden state of the GRU as the query for the attention mechanism\n",
    "        last_hidden = gru_out[:, -1, :] # [batch_size, 2 * hidden_dim]\n",
    "        \n",
    "        # Apply attention to the GRU outputs\n",
    "        context = self.attn(last_hidden, gru_out) # context: [batch_size, 2 * hidden_dim]\n",
    "        \n",
    "        # Pass the context vector through a fully connected layer\n",
    "        output = self.fc(context) # [batch_size, output_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_attn = 128\n",
    "lr_attn=0.001\n",
    "dropout_rate_attn=0.3\n",
    "bs_attn=32\n",
    "output_dim_attn = 1 \n",
    "\n",
    "\n",
    "model_attn= BiGRUWithAttention(embedding_matrix_tensor, hidden_dim_attn, output_dim_attn, dropout_rate_attn).to(device)\n",
    "criterion_attn = nn.BCEWithLogitsLoss()\n",
    "optimizer_attn = torch.optim.Adam(model_attn.parameters(), lr=1e-3)\n",
    "scheduler_attn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attn, mode='min', factor=0.1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.546, Accuracy = 0.772, Val Loss = 0.474 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.350, Accuracy = 0.771, Val Loss = 0.492 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.217, Accuracy = 0.777, Val Loss = 0.565 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.133, Accuracy = 0.770, Val Loss = 0.674 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.075, Accuracy = 0.752, Val Loss = 0.928 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.043, Accuracy = 0.763, Val Loss = 0.903 Learning Rate: 0.000100\n",
      "Epoch 7: Train Loss = 0.024, Accuracy = 0.763, Val Loss = 0.947 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.021, Accuracy = 0.761, Val Loss = 0.995 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.018, Accuracy = 0.762, Val Loss = 1.032 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7617260787992496"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validate(20, model_attn, train_iterator,valid_iterator, optimizer_attn, criterion_attn, scheduler_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.781\n"
     ]
    }
   ],
   "source": [
    "test(model_attn, test_iterator, criterion_attn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
