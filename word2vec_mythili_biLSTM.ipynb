{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "  File \"/var/folders/rw/hx2p35lj2fb3nx3xpvyr3b940000gn/T/ipykernel_45773/2373811666.py\", line 1, in <module>\n",
      "    get_ipython().run_line_magic('pip', 'install torch gensim datasets nltk')\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2480, in run_line_magic\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/magics/packaging.py\", line 105, in pip\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 657, in system_piped\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/utils/_process_posix.py\", line 125, in system\n",
      "ModuleNotFoundError: No module named 'pexpect'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1136, in get_records\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch gensim datasets nltk \n",
    "%pip setuptools wheel\n",
    "%pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "  File \"/var/folders/rw/hx2p35lj2fb3nx3xpvyr3b940000gn/T/ipykernel_45773/1038347899.py\", line 6, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1136, in get_records\n",
      "  File \"/Users/mythilimulani/Projects/SC4002-NLP-Group-Project/myenv/lib/python3.12/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import nltk\n",
    "# nltk.download(\"all\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datasets import load_dataset\n",
    "from gensim.models import KeyedVectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8530 sentences\n",
      "Size of validation set: 1066 sentences\n",
      "Size of test set: 1066 sentences\n"
     ]
    }
   ],
   "source": [
    "#Number of sentences in each set \n",
    "print(f\"Size of training set: {train_dataset.num_rows} sentences\")\n",
    "print(f\"Size of validation set: {validation_dataset.num_rows} sentences\")\n",
    "print(f\"Size of test set: {test_dataset.num_rows} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the embedding matrix and word_to_index from the pickle file\n",
    "with open(\"updated_embedding_matrix.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    embedding_matrix = data[\"embeddings\"]\n",
    "    word_to_index = data[\"word_to_index\"]\n",
    "\n",
    "# Convert embedding_matrix to a NumPy array and a PyTorch tensor\n",
    "embedding_matrix_array = np.array(embedding_matrix)\n",
    "embedding_matrix_tensor = torch.tensor(embedding_matrix_array, dtype=torch.float32)\n",
    "\n",
    "print(f\"Loaded embedding matrix with shape: {embedding_matrix_array.shape}\")\n",
    "print(f\"Vocabulary size (word_to_index): {len(word_to_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenise train, validation, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenized_train_texts = []\n",
    "for sentence in train_dataset['text']:\n",
    "    # Tokenize the sentence using spaCy and store tokens as a list of strings\n",
    "    tokens = [token.text for token in nlp(sentence.lower())]\n",
    "    pre_tokenized_train_texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenize validation and test sets\n",
    "pre_tokenized_validation_texts = [[token.text for token in nlp(sentence.lower())] for sentence in validation_dataset['text']]\n",
    "pre_tokenized_test_texts = [[token.text for token in nlp(sentence.lower())] for sentence in test_dataset['text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3.Keeping the above two adjustments, replace your simple RNN model in Part 2 wioth a biLSTM model and biGRU model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# biLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing the train dataset. text -> word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures reproducibility in CUDA operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disables some optimizations to ensure determinism\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels : list[int], vocab : set, embedding_matrix : dict, max_len=30):\n",
    "        self.texts = tokenized_texts\n",
    "        self.labels  = labels\n",
    "        self.vocab = word_to_index\n",
    "        self.embedding_matrix : dict = embedding_matrix\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        vectorized_text = self.vectorize(text)\n",
    "        return torch.tensor(vectorized_text, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "            \n",
    "    def vectorize(self, tokens):\n",
    "        # Convert tokens to their corresponding index in the vocabulary\n",
    "        vectorized = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "        # Pad or truncate to max_len\n",
    "        if len(vectorized) < self.max_len:\n",
    "            vectorized += [self.vocab['<PAD>']] * (self.max_len - len(vectorized))\n",
    "        else:\n",
    "            vectorized = vectorized[:self.max_len]\n",
    "        return vectorized\n",
    "\n",
    "    def build_vocab_dict(self, vocab : set):\n",
    "        if \"<PAD>\" in vocab : vocab.remove(\"<PAD>\")\n",
    "        if \"<UNK>\" in vocab : vocab.remove(\"<UNK>\")\n",
    "        vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "        vocab_dict['<PAD>'] = len(vocab_dict) # Add padding token\n",
    "        vocab_dict['<UNK>'] = len(vocab_dict) # Add unknown token\n",
    "        return vocab_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentBiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SentimentBiLSTM, self).__init__()\n",
    "        # Load pre-trained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size(1), hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # print(\"LSTM out: \", lstm_out.shape)\n",
    "        # Get the final forward and backward hidden states\n",
    "        out = torch.cat((lstm_out[:, -1, :self.lstm.hidden_size], lstm_out[:, 0, self.lstm.hidden_size:]), dim=1)\n",
    "        # print(out)\n",
    "        out = self.dropout(out)\n",
    "        # return self.sigmoid(self.fc(out))\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in iterator:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch).squeeze(1)\n",
    "        loss = criterion(output, y_batch.float())\n",
    "        loss.backward()\n",
    "        # for param in model.parameters():\n",
    "        #     if param.grad is not None:  # Ensure the gradient is not None\n",
    "        #         print(f\"Gradient norm for {param.shape}: {param.grad.data.norm()}\")\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in iterator:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "            output = model(X_batch).squeeze(1)\n",
    "            \n",
    "            probs = model.sigmoid(output)\n",
    "            prediction = (probs >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(output, y_batch.float())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(prediction.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy, epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(num_epochs, model, train_iterator, valid_iterator, optimizer, criterion, scheduler):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "        accuracy , valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if (epoch==0):\n",
    "                best_val_loss = valid_loss\n",
    "                epochs_without_improvement = 0\n",
    "        print(f'Epoch {epoch + 1}: Train Loss = {train_loss:.3f}, Accuracy = {accuracy:.3f}, Val Loss = {valid_loss:.3f} Learning Rate: {scheduler.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        if valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Check for convergence\n",
    "        if epochs_without_improvement >= 8:  # Convergence condition (no improvement for 4 epochs)\n",
    "            print(\"Convergence reached, stopping training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model and run the train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding_matrix : dict[ str , np.ndarray]= np.load(\"embedding_matrix.npy\",allow_pickle='TRUE').item()\n",
    "# embedding_matrix_values = np.array(list(embedding_matrix.values()), dtype=np.float32)\n",
    "# embedding_matrix_tensor = torch.tensor(embedding_matrix_values, dtype=torch.float32)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = SentimentDataset(pre_tokenized_train_texts, train_dataset['label'], word_to_index, embedding_matrix)\n",
    "valid_dataset = SentimentDataset(pre_tokenized_validation_texts, validation_dataset['label'], word_to_index, embedding_matrix)\n",
    "test_dataset = SentimentDataset(pre_tokenized_test_texts, test_dataset['label'], word_to_index, embedding_matrix)\n",
    "# Create data loaders\n",
    "train_iterator = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128  # Adjust as needed\n",
    "output_dim = 1  # Binary sentiment classification\n",
    "model = SentimentBiLSTM(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate=0.0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
    "# Now you can run your training loop\n",
    "train_and_validate(25, model, train_iterator, valid_iterator, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# # Define the hyper-parameter grid\n",
    "# hidden_dims = [64, 128 ,256]\n",
    "# learning_rates = [0.001, 0.005]\n",
    "# dropout_rates = [0.3, 0.5]\n",
    "# batch_sizes = [32, 64]\n",
    "# output_dim = 1 \n",
    "\n",
    "# # Iterate over all combinations of hyper-parameters\n",
    "# for hidden_dim, lr, dropout_rate, bs in itertools.product(hidden_dims, learning_rates, dropout_rates, batch_sizes):\n",
    "#     print(f'Training with hidden_dim={hidden_dim}, lr={lr}, dropout_rate={dropout_rate}, batch_size={bs}')\n",
    "    \n",
    "#     model = SentimentBiLSTM(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     train_iterator = DataLoader(train_dataset, bs)\n",
    "#     valid_iterator = DataLoader(valid_dataset, bs)\n",
    "\n",
    "#     train_and_validate(25, model, train_iterator, valid_iterator, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with best parameters : \n",
    "hidden_dim=64, \n",
    "\n",
    "lr=0.001, \n",
    "\n",
    "dropout_rate=0.3, \n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentimentBiLSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      5\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m----> 7\u001b[0m model_33 \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentBiLSTM\u001b[49m(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_33\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentimentBiLSTM' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_dim = 64\n",
    "lr=0.001\n",
    "dropout_rate=0.3\n",
    "bs=64\n",
    "output_dim = 1 \n",
    "\n",
    "model = SentimentBiLSTM(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "train_iterator = DataLoader(train_dataset, bs)\n",
    "valid_iterator = DataLoader(valid_dataset, bs)\n",
    "\n",
    "train_and_validate(25, model, train_iterator, valid_iterator, optimizer, criterion, scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Vocab set to dictionary \n",
    "def build_vocab_dict(vocab_set):\n",
    "    # Create the vocabulary dictionary without <PAD> and <UNK>\n",
    "    vocab_set.discard(\"<PAD>\")\n",
    "    vocab_set.discard(\"<UNK>\")\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab_set, start=2)}\n",
    "\n",
    "    # Check for <PAD> and <UNK> existence and assign them fixed indices if they are present\n",
    "    if \"<PAD>\" not in vocab_dict:\n",
    "        vocab_dict[\"<PAD>\"] = 0  # Index for padding token\n",
    "    if \"<UNK>\" not in vocab_dict:\n",
    "        vocab_dict[\"<UNK>\"] = 1  # Index for unknown token\n",
    "    \n",
    "    #add the <PAD> and <UNK> back to the vocab\n",
    "    vocab_set.add(\"<PAD>\")\n",
    "    vocab_set.add(\"<UNK>\")\n",
    "    return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_33' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m sample_tensor \u001b[38;5;241m=\u001b[39m sample_tensor\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move to GPU if available\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Make prediction using the model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel_33\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to compute gradients during inference\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     output \u001b[38;5;241m=\u001b[39m model_33(sample_tensor)  \u001b[38;5;66;03m# Pass the tensor to the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_33' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 8: Get a sample sentence from the test set and predict\n",
    "import random\n",
    "# Select a random index from the test dataset\n",
    "random_index = random.randint(0, len(test_dataset) - 1)\n",
    "\n",
    "# Get the corresponding sentence and its label from the test dataset\n",
    "sample_sentence = test_dataset[random_index]['text']  # Assuming the dataset contains a 'text' field\n",
    "true_label = test_dataset[random_index]['label']  # Assuming there's a label field\n",
    "# Tokenize the sample sentence\n",
    "sample_tokens = nlp(sample_sentence.lower())\n",
    "\n",
    "vocab = word_to_index\n",
    "# Convert tokens to indices\n",
    "sample_indices = pre_tokenized_test_texts[random_index]\n",
    "sample_tensor = torch.tensor(sample_indices).unsqueeze(0)  # Add batch dimension\n",
    "sample_tensor = sample_tensor.to(device)  # Move to GPU if available\n",
    "# Make prediction using the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to compute gradients during inference\n",
    "    output = model(sample_tensor)  # Pass the tensor to the model\n",
    "    print(output)\n",
    "    probs = model.sigmoid(output)\n",
    "    predicted = (probs >= 0.5)\n",
    "    print(predicted.item())\n",
    "\n",
    "# Map predicted index to sentiment label\n",
    "sentiment_labels = ['negative', 'positive']  # Adjust according to your label encoding\n",
    "predicted_label = sentiment_labels[predicted]\n",
    "\n",
    "# Print results\n",
    "print(f\"Sample Sentence: '{sample_sentence}'\")\n",
    "print(f\"True Label: {sentiment_labels[true_label]}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in iterator:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "            output = model(X_batch).squeeze(1)\n",
    "            \n",
    "            probs = model.sigmoid(output)\n",
    "            prediction = (probs >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(output, y_batch.float())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(prediction.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "            \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy = {accuracy:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.744\n"
     ]
    }
   ],
   "source": [
    "test(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM with an Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: [batch_size, hidden_dim] (last hidden state of the decoder or a step in the sequence)\n",
    "        encoder_outputs: [batch_size, seq_len, hidden_dim] (outputs from the encoder)\n",
    "        \"\"\"\n",
    "        # Compute the attention weights using the query (hidden state) and keys (encoder outputs)\n",
    "        attn_weights = torch.matmul(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        # Apply softmax to get the attention weights\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        # Compute the weighted sum of the encoder outputs (values)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout_rate):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        # Load pre-trained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size(1), hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.attn = Attention(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x) #[B, S, E]\n",
    "        \n",
    "        # Get the outputs and hidden states from the LSTM\n",
    "        lstm_out,_= self.lstm(embedded)  # lstm_out: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Use the last hidden state of the LSTM as the query for the attention mechanism\n",
    "        last_hidden = lstm_out[:, -1, :] #[B, 2 * H]\n",
    "        \n",
    "        # Apply attention to the LSTM outputs\n",
    "        context, attn_weights = self.attn(last_hidden, lstm_out) #[B, 2 * H], [B, S]\n",
    "        \n",
    "        # Pass the context vector through a fully connected layer\n",
    "        output = self.fc(context) #[B, output_dim]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "lr=0.001\n",
    "dropout_rate=0.3\n",
    "bs=64\n",
    "output_dim = 1 \n",
    "\n",
    "\n",
    "model_attn= BiLSTMWithAttention(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate ).to(device)\n",
    "criterion_attn = nn.BCEWithLogitsLoss()\n",
    "optimizer_attn = torch.optim.Adam(model_attn.parameters(), lr=1e-3)\n",
    "scheduler_attn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attn, mode='min', factor=0.1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.629, Accuracy = 0.738, Val Loss = 0.532 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.330, Accuracy = 0.748, Val Loss = 0.558 Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "train_and_validate(2, model_attn, train_iterator,valid_iterator, optimizer_attn, criterion_attn, scheduler_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.774\n"
     ]
    }
   ],
   "source": [
    "test(model_attn, test_iterator, criterion_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.012, Accuracy = 0.743, Val Loss = 1.345 Learning Rate: 0.000100\n",
      "Epoch 2: Train Loss = 0.009, Accuracy = 0.741, Val Loss = 1.395 Learning Rate: 0.000100\n",
      "Accuracy = 0.776\n"
     ]
    }
   ],
   "source": [
    "train_and_validate(2, model_attn, train_iterator,valid_iterator, optimizer_attn, criterion_attn, scheduler_attn)\n",
    "test(model_attn, test_iterator, criterion_attn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
