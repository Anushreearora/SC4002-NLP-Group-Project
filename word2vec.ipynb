{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.26.1)\n",
      "Requirement already satisfied: packaging in /Users/nitikaborkar/Library/Python/3.12/lib/python/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nitikaborkar/Library/Python/3.12/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nitikaborkar/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch gensim datasets nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "#nltk.download(\"all\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "\n",
    "from datasets import load_dataset\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8530 sentences\n",
      "Size of validation set: 1066 sentences\n",
      "Size of test set: 1066 sentences\n"
     ]
    }
   ],
   "source": [
    "#Number of sentences in each set \n",
    "print(f\"Size of training set: {train_dataset.num_rows} sentences\")\n",
    "print(f\"Size of validation set: {validation_dataset.num_rows} sentences\")\n",
    "print(f\"Size of test set: {test_dataset.num_rows} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence from train dataset: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
      "Label: Positive\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample sentence from train dataset: {test_dataset[0]['text']}\")\n",
    "print(f\"Label: {'Positive' if test_dataset[0]['label'] == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) What is the size of the vocabulary formed in your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence: ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', '``', 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'] \n",
      "\n",
      "Number of words in the vocabulary(including padding and unknown tokens): 18031\n",
      "Number of words in the vocabulary: 18029\n"
     ]
    }
   ],
   "source": [
    "#tokenize sentences \n",
    "train_tokenized = []\n",
    "for sentence in train_dataset['text']:\n",
    "    train_tokenized.append(word_tokenize(sentence.lower()))\n",
    "\n",
    "print('sample sentence:', train_tokenized[0],'\\n')\n",
    "\n",
    "#build vocabulary\n",
    "vocab = {\"<PAD>\", \"<UNK>\"} #include a padding and unknown token for future processing\n",
    "vocab.update(word for sentence in train_tokenized for word in sentence)\n",
    "\n",
    "print(\"Number of words in the vocabulary(including padding and unknown tokens):\", len(vocab))\n",
    "print(\"Number of words in the vocabulary:\" , len(vocab)-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove). Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you think is the best strategy to mitigate such limitation? Implement your solution in your source code. Show the corresponding code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words with Word2Vec: 3612\n",
      "Embedding for <PAD>: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Embedding for <UNK>: [ 0.03162138 -0.12713236 -0.18221463 -0.02303362  0.12528652  0.06803263\n",
      "  0.10265052  0.06091937 -0.04469366 -0.01014356  0.10413518 -0.2262197\n",
      " -0.23931279 -0.20927985  0.0618528  -0.06083215 -0.0222481   0.02802086\n",
      "  0.15413447  0.17888135 -0.19206112  0.01413485 -0.17839127  0.20454704\n",
      " -0.15227938  0.13267795 -0.13542792 -0.20521541 -0.05059227 -0.02777718\n",
      "  0.22839337 -0.18875864  0.19918136 -0.20078673  0.02072056  0.13459374\n",
      " -0.18508883 -0.16676553  0.1333629   0.06606115  0.10613931 -0.12123027\n",
      " -0.16321254 -0.11630188 -0.02706947  0.03193807 -0.0277745  -0.02810941\n",
      " -0.1551654  -0.18772183  0.15852234 -0.23628023 -0.05153835  0.01546289\n",
      "  0.13176712  0.19028628 -0.0498033   0.00365485  0.17312105 -0.17690392\n",
      "  0.22064674  0.06923739  0.22680931 -0.15252914 -0.12015686  0.08445207\n",
      "  0.2237398   0.24385214  0.14164105  0.10064067  0.08880579  0.10451562\n",
      "  0.04281866  0.05706266 -0.20319423 -0.16137312 -0.09131291 -0.11740985\n",
      " -0.19716307  0.19775326  0.18505892 -0.16445463 -0.00808936  0.07688608\n",
      "  0.15226841  0.18302039 -0.0372902  -0.18708757 -0.04305053 -0.08879607\n",
      "  0.13574397  0.22785495  0.08378254  0.20536299 -0.03352796 -0.09557925\n",
      " -0.04712744 -0.04916304 -0.12212465 -0.06016317  0.08139389 -0.01179195\n",
      " -0.05927296  0.03850042  0.0575338   0.23293538 -0.10859316  0.06016085\n",
      "  0.10425989  0.02239055  0.06962873  0.05696457  0.16191022 -0.1937033\n",
      "  0.01320452  0.14831392 -0.12682001  0.22035738  0.06765955  0.07961689\n",
      " -0.21932896 -0.17745784  0.13334098 -0.11676115  0.14404732  0.07829434\n",
      " -0.0544157   0.22943313  0.12076096 -0.0080485  -0.15866034  0.03398872\n",
      "  0.2172665   0.01618505 -0.15806042 -0.20670613  0.01764867  0.03647991\n",
      " -0.06577573 -0.13342901  0.09302776 -0.11587835 -0.13301411  0.02879848\n",
      " -0.21371348  0.22874226 -0.14034057 -0.07678319 -0.06966598  0.18213629\n",
      " -0.13359943 -0.03479902  0.11233439 -0.0284704  -0.03823866  0.16768966\n",
      " -0.23010512  0.21726653  0.08030604 -0.00681934 -0.18858972  0.05747086\n",
      " -0.08618549 -0.12354012  0.14547664 -0.11119918  0.15815779  0.21206084\n",
      "  0.00073396  0.16966153  0.00708385  0.24464325  0.17004112 -0.02589773\n",
      "  0.02932753  0.2448359   0.15861051  0.11977965 -0.2123142   0.12087804\n",
      " -0.21462293 -0.0729616  -0.13556273  0.21272123  0.00718296  0.1832562\n",
      " -0.04629775  0.18170954  0.10229117  0.17559527  0.05838217  0.16902424\n",
      "  0.2101858  -0.11627405 -0.19648991 -0.05211654  0.05220534  0.15367132\n",
      " -0.19789761 -0.2365626  -0.23562656 -0.07777482 -0.18321716  0.22246955\n",
      "  0.003658    0.15672799  0.21402691  0.04044156  0.0703949   0.03758774\n",
      "  0.20226975 -0.16728068  0.1425516   0.0382918  -0.22796901 -0.09081293\n",
      "  0.05066122 -0.23740254  0.19089249  0.2146655  -0.18582558 -0.09775627\n",
      " -0.12659205  0.23067165 -0.05919258  0.23880322 -0.03176463 -0.12504252\n",
      " -0.24743347  0.12583251 -0.15015332  0.19998638  0.2175873  -0.01819247\n",
      "  0.01026876  0.16337121  0.15735528  0.1869241   0.12868072  0.20584016\n",
      " -0.04880115  0.21352128 -0.22029053  0.12598405  0.0942758   0.08143633\n",
      "  0.17214413  0.14754738 -0.15717493 -0.18433923 -0.17150064 -0.09046553\n",
      " -0.24950978 -0.2436312  -0.18150606 -0.22332158  0.06398141 -0.07933393\n",
      "  0.24227938  0.10631866 -0.13396004 -0.21145736  0.22896821 -0.00675938\n",
      "  0.09185997  0.18790307 -0.14427944  0.05934744 -0.24683344  0.08659161\n",
      "  0.20151166  0.16542527 -0.05122355 -0.12916434 -0.19077265  0.07299577\n",
      "  0.21654397  0.09289731 -0.04109672  0.02059531 -0.14481613  0.16694803\n",
      " -0.12927165  0.12423473  0.01112249  0.1338043  -0.22371619 -0.0413856\n",
      " -0.02258096  0.10020691 -0.05951557 -0.22639274 -0.01023111 -0.23803267\n",
      " -0.16200385 -0.2361323   0.14160106 -0.21898613 -0.07295655  0.08889401]\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Word2Vec model (Google News Word2Vec)\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Set embedding size\n",
    "embedding_size = 300\n",
    "\n",
    "# Initialize the embedding matrix with zeros for padding and random values for unknown tokens\n",
    "embedding_matrix = {}\n",
    "\n",
    "# Create an <UNK> token embedding as a random vector\n",
    "unk_vector = np.random.uniform(-0.25, 0.25, embedding_size)\n",
    "embedding_matrix[\"<UNK>\"] = unk_vector\n",
    "\n",
    "# Create a <PAD> token embedding as a zero vector\n",
    "pad_vector = np.zeros(embedding_size)\n",
    "embedding_matrix[\"<PAD>\"] = pad_vector\n",
    "\n",
    "# Initialize OOV counter\n",
    "oov_count = 0\n",
    "\n",
    "# Iterate over the vocabulary\n",
    "for word in vocab:\n",
    "    if word == \"<PAD>\" or word == \"<UNK>\":\n",
    "        continue  \n",
    "    \n",
    "    if word in word2vec:  # If the word is in Word2Vec, add its embedding\n",
    "        embedding_matrix[word] = word2vec[word]\n",
    "    else:\n",
    "        # If the word is OOV, assign it the <UNK> vector and count as OOV\n",
    "        embedding_matrix[word] = unk_vector  # Assign OOV words the <UNK> vector\n",
    "        oov_count += 1  # Increment OOV counter\n",
    "\n",
    "# Print results for Word2Vec\n",
    "print(f\"Number of OOV words with Word2Vec: {oov_count}\")\n",
    "print(f\"Embedding for <PAD>: {embedding_matrix['<PAD>']}\")\n",
    "print(f\"Embedding for <UNK>: {embedding_matrix['<UNK>']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words with FastText: 1961\n",
      "Embedding for <PAD>: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Embedding for <UNK>: [-0.00215902 -0.22117789 -0.15893889  0.24906639 -0.01257831 -0.112415\n",
      " -0.19281839 -0.09669723 -0.10310026 -0.11392947 -0.1092611  -0.20160805\n",
      "  0.00415984 -0.05864284 -0.0801247   0.06211952  0.18221996 -0.2450124\n",
      "  0.20235944  0.03646764 -0.11308073  0.09611091 -0.14002638 -0.12297463\n",
      "  0.23181831  0.02168948  0.05102684  0.04423168  0.18939601  0.18387225\n",
      " -0.2309906  -0.03090928 -0.19412582  0.15880006  0.03553606 -0.07441696\n",
      " -0.1663649  -0.10228346 -0.17592401 -0.05695692  0.09482358  0.07961092\n",
      " -0.20529605 -0.06593214 -0.17805874  0.08073377  0.19811845 -0.23183754\n",
      " -0.217198    0.14022974  0.18600338 -0.2058712   0.02342809 -0.15021733\n",
      " -0.01658101 -0.09851484  0.00532675  0.14692549 -0.08323109 -0.04246608\n",
      "  0.10045606  0.00159372  0.16460424 -0.17512149 -0.19108489  0.16950003\n",
      "  0.19660676  0.22749302 -0.11396804  0.06671527 -0.02390948  0.04055565\n",
      "  0.24761812  0.03556454 -0.22932335  0.02873758 -0.22326827 -0.21954005\n",
      "  0.00614663 -0.21785388 -0.10423518 -0.20693191 -0.13349518 -0.08757862\n",
      " -0.02459211  0.22374953  0.0207414  -0.22895403  0.22424858 -0.21921598\n",
      " -0.05663246 -0.11190164  0.16302604  0.14442924 -0.15845908  0.09892239\n",
      " -0.20732845  0.22659026 -0.24447914 -0.16156436 -0.16589459  0.08272596\n",
      " -0.23704629  0.06352216  0.11440614  0.06000492 -0.17724353 -0.05268178\n",
      "  0.13706786  0.12450818 -0.23952485  0.24451585  0.06775694 -0.19121762\n",
      " -0.24543181 -0.24091732  0.12985452  0.04618928  0.01605106 -0.00780997\n",
      "  0.20896999  0.09892749  0.02748464 -0.08300794 -0.13110649 -0.07231946\n",
      " -0.0879473   0.05119221 -0.07605654  0.03060656  0.09753512  0.08636757\n",
      "  0.22809318 -0.19198314  0.16299999  0.204812   -0.16702383 -0.16950508\n",
      " -0.20308076 -0.15471385 -0.04970623  0.11684963 -0.01933067 -0.0637507\n",
      " -0.08158852  0.14042392 -0.08045745  0.05413653 -0.03112351 -0.16493491\n",
      "  0.13630987  0.03816261 -0.24215699  0.12537323  0.1752172  -0.10838156\n",
      " -0.21399388 -0.0862424   0.06817854 -0.08475672  0.22783248 -0.0189971\n",
      " -0.09390722  0.1230889   0.20445672 -0.11327082  0.21185921 -0.08179003\n",
      " -0.22441581  0.13217966  0.13201953 -0.16089882  0.1854437   0.13248523\n",
      "  0.21692683 -0.08606394  0.05047127 -0.14543461 -0.01744864 -0.15912427\n",
      "  0.09743082 -0.22366162 -0.23428375 -0.19442573 -0.1519836  -0.0020344\n",
      " -0.1158717  -0.02294466 -0.18617413  0.1294477  -0.13871985  0.03228283\n",
      " -0.21665222  0.07214006  0.09124736 -0.19166021  0.02771552 -0.12710946\n",
      "  0.21745699 -0.16802507 -0.07962628  0.23829311 -0.19537497 -0.24493516\n",
      "  0.04200118 -0.16963206  0.10442824 -0.11624543 -0.2296903  -0.12101613\n",
      "  0.14586708 -0.19945641 -0.06534461 -0.02127494  0.06467736 -0.08968013\n",
      "  0.11531175 -0.12836108  0.12344506  0.15002883 -0.08177148 -0.19232099\n",
      " -0.10586738 -0.10061333  0.07934285 -0.08065541 -0.19754477 -0.01826874\n",
      " -0.02642239 -0.14280117 -0.22226924 -0.20260155 -0.05248633 -0.07353991\n",
      "  0.09606818 -0.20729632  0.20830766  0.12402198  0.20965077 -0.04410811\n",
      " -0.00246784 -0.08068941  0.15598105 -0.19804379 -0.14008183 -0.16268084\n",
      "  0.04551675  0.06207561  0.14625095  0.12398789  0.11830276 -0.00426306\n",
      "  0.21190077 -0.03015664 -0.11315354 -0.00636708  0.17516376  0.20722695\n",
      "  0.0593836   0.22263088  0.05571496 -0.05961168 -0.21425039 -0.05987328\n",
      "  0.03234028 -0.11678504 -0.2274066  -0.03309198 -0.04781708  0.03746268\n",
      "  0.01956228  0.12012934 -0.24372483 -0.04027269 -0.09453317 -0.13933844\n",
      " -0.17356891 -0.14185187 -0.06408491  0.15104763 -0.15142527 -0.1265852\n",
      "  0.00036873 -0.1422262  -0.08413457  0.05343364 -0.04858118  0.15126261\n",
      "  0.02406007 -0.20627571 -0.21280926 -0.03848495  0.03773007  0.10651376\n",
      "  0.24886158 -0.0722994  -0.09911766  0.1023795   0.02919719 -0.14864191]\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained FastText model (wiki-news-300d-subword)\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Set embedding size\n",
    "embedding_size = 300\n",
    "\n",
    "# Initialize the embedding matrix with zeros for padding and random values for unknown tokens\n",
    "embedding_matrix = {}\n",
    "\n",
    "# Create an <UNK> token embedding as a random vector\n",
    "unk_vector = np.random.uniform(-0.25, 0.25, embedding_size)\n",
    "embedding_matrix[\"<UNK>\"] = unk_vector\n",
    "\n",
    "# Create a <PAD> token embedding as a zero vector\n",
    "pad_vector = np.zeros(embedding_size)\n",
    "embedding_matrix[\"<PAD>\"] = pad_vector\n",
    "\n",
    "# Initialize OOV counter for FastText\n",
    "oov_count_fasttext = 0\n",
    "\n",
    "# Iterate over the vocabulary\n",
    "for word in vocab:\n",
    "    if word == \"<PAD>\" or word == \"<UNK>\":\n",
    "        continue  \n",
    "    \n",
    "    try:\n",
    "        # Try to get the word vector using FastText's subword handling\n",
    "        embedding_matrix[word] = fasttext_model.get_vector(word)\n",
    "    except KeyError:\n",
    "        # If the word can't be processed even by FastText, assign it the <UNK> vector\n",
    "        embedding_matrix[word] = unk_vector\n",
    "        oov_count_fasttext += 1  # Increment OOV count\n",
    "\n",
    "# Print results for FastText\n",
    "print(f\"Number of OOV words with FastText: {oov_count_fasttext}\")\n",
    "print(f\"Embedding for <PAD>: {embedding_matrix['<PAD>']}\")\n",
    "print(f\"Embedding for <UNK>: {embedding_matrix['<UNK>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the embeddings so that they can be used later\n",
    "np.save(\"embedding_matrix.npy\", embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Model Training & Evaluation - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.load(\"embedding_matrix.npy\",allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Vocab set to dictionary \n",
    "def build_vocab_dict(vocab_set):\n",
    "    # Create the vocabulary dictionary without <PAD> and <UNK>\n",
    "    vocab_set.discard(\"<PAD>\")\n",
    "    vocab_set.discard(\"<UNK>\")\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab_set, start=2)}\n",
    "\n",
    "    # Check for <PAD> and <UNK> existence and assign them fixed indices if they are present\n",
    "    if \"<PAD>\" not in vocab_dict:\n",
    "        vocab_dict[\"<PAD>\"] = 0  # Index for padding token\n",
    "    if \"<UNK>\" not in vocab_dict:\n",
    "        vocab_dict[\"<UNK>\"] = 1  # Index for unknown token\n",
    "    \n",
    "    #add the <PAD> and <UNK> back to the vocab\n",
    "    vocab_set.add(\"<PAD>\")\n",
    "    vocab_set.add(\"<UNK>\")\n",
    "    return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset for PyTorch\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, embedding_matrix, max_len=30):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = build_vocab_dict(vocab)\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = word_tokenize(text.lower())\n",
    "        vectorized_text = self.vectorize(tokenized_text)\n",
    "        return torch.tensor(vectorized_text), torch.tensor(label)\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        vectorized = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "        # Check for out-of-range indices\n",
    "        for index in vectorized:\n",
    "            if index >= len(self.embedding_matrix):\n",
    "                raise ValueError(f\"Index {index} is out of range for the embedding matrix.\")\n",
    "                \n",
    "        # Pad or truncate to max_len\n",
    "        if len(vectorized) < self.max_len:\n",
    "            vectorized += [self.vocab['<PAD>']] * (self.max_len - len(vectorized))\n",
    "        else:\n",
    "            vectorized = vectorized[:self.max_len]\n",
    "        return vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, output_size, num_layers=2, bidirectional=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Define the embedding layer with pretrained embeddings, frozen\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n",
    "        \n",
    "        # Define RNN layer \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        # Using Average Pooling\n",
    "        out = torch.mean(out, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "def create_data_loader(dataset, batch_size):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset_instance = SentimentDataset(train_dataset['text'], train_dataset['label'], vocab, embedding_matrix)\n",
    "val_dataset_instance = SentimentDataset(validation_dataset['text'], validation_dataset['label'], vocab, embedding_matrix)\n",
    "test_dataset_instance = SentimentDataset(test_dataset['text'], test_dataset['label'], vocab, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            output = model.forward(data)\n",
    "            probs = torch.sigmoid(output)  # Apply sigmoid to get probabilities\n",
    "            predicted = (probs >= 0.5).long()  # Convert probabilities to binary predictions\n",
    "            all_preds.extend(predicted.cpu().numpy().flatten().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate function\n",
    "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, max_epochs=100, convergence_threshold=0.001):\n",
    "    best_val_acc = 0\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).squeeze(1)\n",
    "            loss = criterion(output, target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        val_acc = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {running_loss/len(train_loader)}, Val Accuracy: {val_acc}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Check for convergence\n",
    "        if epochs_without_improvement >= 10:  # Convergence condition (no improvement for 5 epochs)\n",
    "            print(\"Convergence reached, stopping training.\")\n",
    "            break\n",
    "            \n",
    "    return best_val_acc, epoch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 32, Hidden Size: 123, Optimizer: adam\n",
      "Epoch 1/100, Loss: 0.6943659646234263, Val Accuracy: 0.5412757973733584\n",
      "Epoch 2/100, Loss: 0.6830613117539481, Val Accuracy: 0.549718574108818\n",
      "Epoch 3/100, Loss: 0.6558597842405798, Val Accuracy: 0.6097560975609756\n",
      "Epoch 4/100, Loss: 0.635233868597152, Val Accuracy: 0.6341463414634146\n",
      "Epoch 5/100, Loss: 0.6168298465705543, Val Accuracy: 0.6622889305816135\n",
      "Epoch 6/100, Loss: 0.6045206906866938, Val Accuracy: 0.6378986866791745\n",
      "Epoch 7/100, Loss: 0.6005654070484504, Val Accuracy: 0.6388367729831145\n",
      "Epoch 8/100, Loss: 0.5948330480954174, Val Accuracy: 0.6341463414634146\n",
      "Epoch 9/100, Loss: 0.5995116915818903, Val Accuracy: 0.649155722326454\n",
      "Epoch 10/100, Loss: 0.5876737326271971, Val Accuracy: 0.6529080675422139\n",
      "Epoch 11/100, Loss: 0.590491944819354, Val Accuracy: 0.624765478424015\n",
      "Epoch 12/100, Loss: 0.5891645145550203, Val Accuracy: 0.6604127579737336\n",
      "Epoch 13/100, Loss: 0.5820946382002884, Val Accuracy: 0.6407129455909943\n",
      "Epoch 14/100, Loss: 0.5795437009370282, Val Accuracy: 0.6622889305816135\n",
      "Epoch 15/100, Loss: 0.5809716669361243, Val Accuracy: 0.6848030018761726\n",
      "Epoch 16/100, Loss: 0.5688756120562107, Val Accuracy: 0.626641651031895\n",
      "Epoch 17/100, Loss: 0.5767360127597266, Val Accuracy: 0.6716697936210131\n",
      "Epoch 18/100, Loss: 0.5693674433543887, Val Accuracy: 0.6585365853658537\n",
      "Epoch 19/100, Loss: 0.5636264330215668, Val Accuracy: 0.6754221388367729\n",
      "Epoch 20/100, Loss: 0.5694116739074836, Val Accuracy: 0.6604127579737336\n",
      "Epoch 21/100, Loss: 0.5675148276354043, Val Accuracy: 0.6744840525328331\n",
      "Epoch 22/100, Loss: 0.5627601986074269, Val Accuracy: 0.6679174484052532\n",
      "Epoch 23/100, Loss: 0.5772750091909916, Val Accuracy: 0.6322701688555347\n",
      "Epoch 24/100, Loss: 0.5803011890207783, Val Accuracy: 0.6632270168855535\n",
      "Epoch 25/100, Loss: 0.5745503938153442, Val Accuracy: 0.6669793621013134\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 32 Optimizer: adam, Validation Accuracy: 0.6848030018761726\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 32, Hidden Size: 123, Optimizer: sgd\n",
      "Epoch 1/100, Loss: 0.6930751166540139, Val Accuracy: 0.5159474671669794\n",
      "Epoch 2/100, Loss: 0.6930679408798504, Val Accuracy: 0.5300187617260788\n",
      "Epoch 3/100, Loss: 0.693063491515899, Val Accuracy: 0.5290806754221389\n",
      "Epoch 4/100, Loss: 0.6930604248904111, Val Accuracy: 0.5403377110694184\n",
      "Epoch 5/100, Loss: 0.693054746599233, Val Accuracy: 0.5375234521575984\n",
      "Epoch 6/100, Loss: 0.6930517717247152, Val Accuracy: 0.5262664165103189\n",
      "Epoch 7/100, Loss: 0.6930450598398844, Val Accuracy: 0.5121951219512195\n",
      "Epoch 8/100, Loss: 0.6930399079447829, Val Accuracy: 0.5028142589118199\n",
      "Epoch 9/100, Loss: 0.6930411935745554, Val Accuracy: 0.5065666041275797\n",
      "Epoch 10/100, Loss: 0.6930316041471359, Val Accuracy: 0.5103189493433395\n",
      "Epoch 11/100, Loss: 0.69303420018614, Val Accuracy: 0.5075046904315197\n",
      "Epoch 12/100, Loss: 0.6930257579807039, Val Accuracy: 0.5178236397748592\n",
      "Epoch 13/100, Loss: 0.6930209791169184, Val Accuracy: 0.524390243902439\n",
      "Epoch 14/100, Loss: 0.6930199754372072, Val Accuracy: 0.5290806754221389\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 32 Optimizer: sgd, Validation Accuracy: 0.5403377110694184\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 32, Hidden Size: 123, Optimizer: rmsprop\n",
      "Epoch 1/100, Loss: 0.6985611647702334, Val Accuracy: 0.4793621013133208\n",
      "Epoch 2/100, Loss: 0.6890343702241276, Val Accuracy: 0.5412757973733584\n",
      "Epoch 3/100, Loss: 0.691692861055167, Val Accuracy: 0.5234521575984991\n",
      "Epoch 4/100, Loss: 0.689089277710361, Val Accuracy: 0.5365853658536586\n",
      "Epoch 5/100, Loss: 0.6883157083157743, Val Accuracy: 0.5262664165103189\n",
      "Epoch 6/100, Loss: 0.6858479103792027, Val Accuracy: 0.525328330206379\n",
      "Epoch 7/100, Loss: 0.6962863651107759, Val Accuracy: 0.5384615384615384\n",
      "Epoch 8/100, Loss: 0.6878348644753074, Val Accuracy: 0.5384615384615384\n",
      "Epoch 9/100, Loss: 0.6901397240742315, Val Accuracy: 0.49624765478424016\n",
      "Epoch 10/100, Loss: 0.6870403861285149, Val Accuracy: 0.5431519699812383\n",
      "Epoch 11/100, Loss: 0.6874938535779603, Val Accuracy: 0.5356472795497186\n",
      "Epoch 12/100, Loss: 0.6783879412693924, Val Accuracy: 0.5647279549718575\n",
      "Epoch 13/100, Loss: 0.6611934982882011, Val Accuracy: 0.5928705440900562\n",
      "Epoch 14/100, Loss: 0.6437595783547962, Val Accuracy: 0.5947467166979362\n",
      "Epoch 15/100, Loss: 0.6261452498953887, Val Accuracy: 0.5891181988742964\n",
      "Epoch 16/100, Loss: 0.6184466150816014, Val Accuracy: 0.6200750469043153\n",
      "Epoch 17/100, Loss: 0.6068683431836103, Val Accuracy: 0.6566604127579737\n",
      "Epoch 18/100, Loss: 0.6054451112890065, Val Accuracy: 0.6378986866791745\n",
      "Epoch 19/100, Loss: 0.6023172661383054, Val Accuracy: 0.6697936210131332\n",
      "Epoch 20/100, Loss: 0.6007736270570576, Val Accuracy: 0.6210131332082551\n",
      "Epoch 21/100, Loss: 0.5937234944618597, Val Accuracy: 0.6651031894934334\n",
      "Epoch 22/100, Loss: 0.5918877324584718, Val Accuracy: 0.6566604127579737\n",
      "Epoch 23/100, Loss: 0.586646544129661, Val Accuracy: 0.6697936210131332\n",
      "Epoch 24/100, Loss: 0.582335978634795, Val Accuracy: 0.6632270168855535\n",
      "Epoch 25/100, Loss: 0.5790170353003655, Val Accuracy: 0.6660412757973734\n",
      "Epoch 26/100, Loss: 0.5762646584475085, Val Accuracy: 0.6538461538461539\n",
      "Epoch 27/100, Loss: 0.5776039755969459, Val Accuracy: 0.6782363977485929\n",
      "Epoch 28/100, Loss: 0.5706175430660391, Val Accuracy: 0.6632270168855535\n",
      "Epoch 29/100, Loss: 0.57859826590238, Val Accuracy: 0.6651031894934334\n",
      "Epoch 30/100, Loss: 0.5766182517291009, Val Accuracy: 0.6191369606003753\n",
      "Epoch 31/100, Loss: 0.5816716345285209, Val Accuracy: 0.6144465290806754\n",
      "Epoch 32/100, Loss: 0.5787886422448391, Val Accuracy: 0.6707317073170732\n",
      "Epoch 33/100, Loss: 0.5776311718346028, Val Accuracy: 0.6594746716697936\n",
      "Epoch 34/100, Loss: 0.5792686060796516, Val Accuracy: 0.6594746716697936\n",
      "Epoch 35/100, Loss: 0.5767233771331302, Val Accuracy: 0.6613508442776735\n",
      "Epoch 36/100, Loss: 0.5734979981340272, Val Accuracy: 0.6707317073170732\n",
      "Epoch 37/100, Loss: 0.5662845300601216, Val Accuracy: 0.6679174484052532\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 32 Optimizer: rmsprop, Validation Accuracy: 0.6782363977485929\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 64, Hidden Size: 123, Optimizer: adam\n",
      "Epoch 1/100, Loss: 0.6936542071513275, Val Accuracy: 0.5412757973733584\n",
      "Epoch 2/100, Loss: 0.680032718092648, Val Accuracy: 0.5975609756097561\n",
      "Epoch 3/100, Loss: 0.6608590919580033, Val Accuracy: 0.6135084427767354\n",
      "Epoch 4/100, Loss: 0.6234295999825891, Val Accuracy: 0.5938086303939962\n",
      "Epoch 5/100, Loss: 0.6140116317503488, Val Accuracy: 0.5947467166979362\n",
      "Epoch 6/100, Loss: 0.6054786955242726, Val Accuracy: 0.6425891181988743\n",
      "Epoch 7/100, Loss: 0.5915020975603986, Val Accuracy: 0.626641651031895\n",
      "Epoch 8/100, Loss: 0.5944594167061706, Val Accuracy: 0.5966228893058161\n",
      "Epoch 9/100, Loss: 0.5927128969733395, Val Accuracy: 0.6641651031894934\n",
      "Epoch 10/100, Loss: 0.5877360574789902, Val Accuracy: 0.6594746716697936\n",
      "Epoch 11/100, Loss: 0.5814011508404319, Val Accuracy: 0.6716697936210131\n",
      "Epoch 12/100, Loss: 0.5793757612135872, Val Accuracy: 0.6669793621013134\n",
      "Epoch 13/100, Loss: 0.5732304318182504, Val Accuracy: 0.6697936210131332\n",
      "Epoch 14/100, Loss: 0.5819719754048248, Val Accuracy: 0.651031894934334\n",
      "Epoch 15/100, Loss: 0.5844131325607869, Val Accuracy: 0.6679174484052532\n",
      "Epoch 16/100, Loss: 0.5799308977465132, Val Accuracy: 0.6660412757973734\n",
      "Epoch 17/100, Loss: 0.586849959928598, Val Accuracy: 0.6444652908067542\n",
      "Epoch 18/100, Loss: 0.5767913316168002, Val Accuracy: 0.6763602251407129\n",
      "Epoch 19/100, Loss: 0.5741848051548004, Val Accuracy: 0.6679174484052532\n",
      "Epoch 20/100, Loss: 0.57543353958806, Val Accuracy: 0.6754221388367729\n",
      "Epoch 21/100, Loss: 0.5766495401734737, Val Accuracy: 0.6397748592870544\n",
      "Epoch 22/100, Loss: 0.569429659354153, Val Accuracy: 0.6632270168855535\n",
      "Epoch 23/100, Loss: 0.5723959074091556, Val Accuracy: 0.6679174484052532\n",
      "Epoch 24/100, Loss: 0.5674554496558745, Val Accuracy: 0.6772983114446529\n",
      "Epoch 25/100, Loss: 0.5692990495642619, Val Accuracy: 0.6360225140712945\n",
      "Epoch 26/100, Loss: 0.5649610106179963, Val Accuracy: 0.651031894934334\n",
      "Epoch 27/100, Loss: 0.5846789761710522, Val Accuracy: 0.6604127579737336\n",
      "Epoch 28/100, Loss: 0.5646161116770844, Val Accuracy: 0.6707317073170732\n",
      "Epoch 29/100, Loss: 0.5617854123684898, Val Accuracy: 0.6716697936210131\n",
      "Epoch 30/100, Loss: 0.5636773794444639, Val Accuracy: 0.6688555347091932\n",
      "Epoch 31/100, Loss: 0.5637664623669724, Val Accuracy: 0.6726078799249531\n",
      "Epoch 32/100, Loss: 0.5590853377509473, Val Accuracy: 0.6604127579737336\n",
      "Epoch 33/100, Loss: 0.5637643590791902, Val Accuracy: 0.6735459662288931\n",
      "Epoch 34/100, Loss: 0.5582823651050454, Val Accuracy: 0.6735459662288931\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 64 Optimizer: adam, Validation Accuracy: 0.6772983114446529\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 64, Hidden Size: 123, Optimizer: sgd\n",
      "Epoch 1/100, Loss: 0.6930833547862608, Val Accuracy: 0.5168855534709194\n",
      "Epoch 2/100, Loss: 0.6930847755118982, Val Accuracy: 0.5187617260787992\n",
      "Epoch 3/100, Loss: 0.6930768672210067, Val Accuracy: 0.5140712945590994\n",
      "Epoch 4/100, Loss: 0.6930819091512196, Val Accuracy: 0.5215759849906192\n",
      "Epoch 5/100, Loss: 0.6930757707624293, Val Accuracy: 0.5178236397748592\n",
      "Epoch 6/100, Loss: 0.6930791201876171, Val Accuracy: 0.5234521575984991\n",
      "Epoch 7/100, Loss: 0.6930694201988961, Val Accuracy: 0.5234521575984991\n",
      "Epoch 8/100, Loss: 0.6930664746618983, Val Accuracy: 0.524390243902439\n",
      "Epoch 9/100, Loss: 0.6930699815501028, Val Accuracy: 0.5225140712945591\n",
      "Epoch 10/100, Loss: 0.6930610048237131, Val Accuracy: 0.5272045028142589\n",
      "Epoch 11/100, Loss: 0.693059701972933, Val Accuracy: 0.5215759849906192\n",
      "Epoch 12/100, Loss: 0.6930559006199908, Val Accuracy: 0.5196998123827392\n",
      "Epoch 13/100, Loss: 0.6930593047569047, Val Accuracy: 0.5206378986866792\n",
      "Epoch 14/100, Loss: 0.6930561608342982, Val Accuracy: 0.5178236397748592\n",
      "Epoch 15/100, Loss: 0.6930495349328909, Val Accuracy: 0.5168855534709194\n",
      "Epoch 16/100, Loss: 0.6930474558872963, Val Accuracy: 0.5196998123827392\n",
      "Epoch 17/100, Loss: 0.6930497377666075, Val Accuracy: 0.5178236397748592\n",
      "Epoch 18/100, Loss: 0.693050001984212, Val Accuracy: 0.5196998123827392\n",
      "Epoch 19/100, Loss: 0.6930469047667375, Val Accuracy: 0.5206378986866792\n",
      "Epoch 20/100, Loss: 0.6930381313188753, Val Accuracy: 0.5225140712945591\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 64 Optimizer: sgd, Validation Accuracy: 0.5272045028142589\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 64, Hidden Size: 123, Optimizer: rmsprop\n",
      "Epoch 1/100, Loss: 0.694106093983152, Val Accuracy: 0.5365853658536586\n",
      "Epoch 2/100, Loss: 0.6839294068848909, Val Accuracy: 0.5525328330206379\n",
      "Epoch 3/100, Loss: 0.6591469151760215, Val Accuracy: 0.5975609756097561\n",
      "Epoch 4/100, Loss: 0.6257915439000771, Val Accuracy: 0.6313320825515948\n",
      "Epoch 5/100, Loss: 0.6150019253367809, Val Accuracy: 0.6313320825515948\n",
      "Epoch 6/100, Loss: 0.6108877609914808, Val Accuracy: 0.599437148217636\n",
      "Epoch 7/100, Loss: 0.5987826087581578, Val Accuracy: 0.6435272045028143\n",
      "Epoch 8/100, Loss: 0.6004217667366142, Val Accuracy: 0.5628517823639775\n",
      "Epoch 9/100, Loss: 0.5985852037347964, Val Accuracy: 0.650093808630394\n",
      "Epoch 10/100, Loss: 0.5978839942323628, Val Accuracy: 0.6332082551594747\n",
      "Epoch 11/100, Loss: 0.5988537904931538, Val Accuracy: 0.6360225140712945\n",
      "Epoch 12/100, Loss: 0.5960920733302387, Val Accuracy: 0.6566604127579737\n",
      "Epoch 13/100, Loss: 0.5908835659276194, Val Accuracy: 0.6369606003752345\n",
      "Epoch 14/100, Loss: 0.5919327273297665, Val Accuracy: 0.5938086303939962\n",
      "Epoch 15/100, Loss: 0.591297215045388, Val Accuracy: 0.6116322701688556\n",
      "Epoch 16/100, Loss: 0.6015456174291781, Val Accuracy: 0.6557223264540337\n",
      "Epoch 17/100, Loss: 0.5889584308684762, Val Accuracy: 0.5938086303939962\n",
      "Epoch 18/100, Loss: 0.5873347385160959, Val Accuracy: 0.5900562851782364\n",
      "Epoch 19/100, Loss: 0.5769511447913611, Val Accuracy: 0.6669793621013134\n",
      "Epoch 20/100, Loss: 0.5785433836837313, Val Accuracy: 0.6735459662288931\n",
      "Epoch 21/100, Loss: 0.5762149429143365, Val Accuracy: 0.6210131332082551\n",
      "Epoch 22/100, Loss: 0.5788240715194104, Val Accuracy: 0.6341463414634146\n",
      "Epoch 23/100, Loss: 0.5774898119826815, Val Accuracy: 0.6547842401500938\n",
      "Epoch 24/100, Loss: 0.5765810831269221, Val Accuracy: 0.6294559099437148\n",
      "Epoch 25/100, Loss: 0.5721033896083263, Val Accuracy: 0.650093808630394\n",
      "Epoch 26/100, Loss: 0.5706846246968454, Val Accuracy: 0.5637898686679175\n",
      "Epoch 27/100, Loss: 0.5684789363572846, Val Accuracy: 0.6613508442776735\n",
      "Epoch 28/100, Loss: 0.5642602703909376, Val Accuracy: 0.6191369606003753\n",
      "Epoch 29/100, Loss: 0.5598948758039901, Val Accuracy: 0.6444652908067542\n",
      "Epoch 30/100, Loss: 0.5611858512483426, Val Accuracy: 0.6566604127579737\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 64 Optimizer: rmsprop, Validation Accuracy: 0.6735459662288931\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 32, Hidden Size: 123, Optimizer: adam\n",
      "Epoch 1/100, Loss: 0.6962930611456825, Val Accuracy: 0.5215759849906192\n",
      "Epoch 2/100, Loss: 0.6921677935435977, Val Accuracy: 0.525328330206379\n",
      "Epoch 3/100, Loss: 0.6927044985446145, Val Accuracy: 0.525328330206379\n",
      "Epoch 4/100, Loss: 0.6891289545355664, Val Accuracy: 0.524390243902439\n",
      "Epoch 5/100, Loss: 0.6895773522416304, Val Accuracy: 0.5121951219512195\n",
      "Epoch 6/100, Loss: 0.6888101353180989, Val Accuracy: 0.5168855534709194\n",
      "Epoch 7/100, Loss: 0.6879154068700383, Val Accuracy: 0.5309568480300187\n",
      "Epoch 8/100, Loss: 0.6860090674978964, Val Accuracy: 0.5403377110694184\n",
      "Epoch 9/100, Loss: 0.6837927136528358, Val Accuracy: 0.5393996247654784\n",
      "Epoch 10/100, Loss: 0.6805543169546663, Val Accuracy: 0.5375234521575984\n",
      "Epoch 11/100, Loss: 0.6776140832275933, Val Accuracy: 0.5628517823639775\n",
      "Epoch 12/100, Loss: 0.6759677931164088, Val Accuracy: 0.5356472795497186\n",
      "Epoch 13/100, Loss: 0.6801878948336684, Val Accuracy: 0.5356472795497186\n",
      "Epoch 14/100, Loss: 0.6769659159781781, Val Accuracy: 0.5309568480300187\n",
      "Epoch 15/100, Loss: 0.6736684957247102, Val Accuracy: 0.5337711069418386\n",
      "Epoch 16/100, Loss: 0.6726473034097907, Val Accuracy: 0.5384615384615384\n",
      "Epoch 17/100, Loss: 0.6697004254391131, Val Accuracy: 0.5318949343339587\n",
      "Epoch 18/100, Loss: 0.6693840683176276, Val Accuracy: 0.5412757973733584\n",
      "Epoch 19/100, Loss: 0.6677787681197406, Val Accuracy: 0.5347091932457786\n",
      "Epoch 20/100, Loss: 0.6661229296569967, Val Accuracy: 0.549718574108818\n",
      "Epoch 21/100, Loss: 0.6652713447027885, Val Accuracy: 0.5393996247654784\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 32 Optimizer: adam, Validation Accuracy: 0.5628517823639775\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 32, Hidden Size: 123, Optimizer: sgd\n",
      "Epoch 1/100, Loss: 0.693415150660254, Val Accuracy: 0.5103189493433395\n",
      "Epoch 2/100, Loss: 0.6932474198412806, Val Accuracy: 0.5\n",
      "Epoch 3/100, Loss: 0.693245095036896, Val Accuracy: 0.5\n",
      "Epoch 4/100, Loss: 0.6931206675504478, Val Accuracy: 0.5\n",
      "Epoch 5/100, Loss: 0.6931779364521584, Val Accuracy: 0.4906191369606004\n",
      "Epoch 6/100, Loss: 0.693107065636567, Val Accuracy: 0.5065666041275797\n",
      "Epoch 7/100, Loss: 0.6930809123685744, Val Accuracy: 0.5272045028142589\n",
      "Epoch 8/100, Loss: 0.6930473915646586, Val Accuracy: 0.49906191369606\n",
      "Epoch 9/100, Loss: 0.6929708259382498, Val Accuracy: 0.5\n",
      "Epoch 10/100, Loss: 0.6929151976153198, Val Accuracy: 0.5\n",
      "Epoch 11/100, Loss: 0.6929522489340564, Val Accuracy: 0.5028142589118199\n",
      "Epoch 12/100, Loss: 0.6928849168931054, Val Accuracy: 0.50187617260788\n",
      "Epoch 13/100, Loss: 0.6928203507755579, Val Accuracy: 0.5\n",
      "Epoch 14/100, Loss: 0.6926799124099788, Val Accuracy: 0.5093808630393997\n",
      "Epoch 15/100, Loss: 0.692726664328843, Val Accuracy: 0.5206378986866792\n",
      "Epoch 16/100, Loss: 0.6925593302044529, Val Accuracy: 0.4971857410881801\n",
      "Epoch 17/100, Loss: 0.6925511797715662, Val Accuracy: 0.5375234521575984\n",
      "Epoch 18/100, Loss: 0.6924459362744392, Val Accuracy: 0.5168855534709194\n",
      "Epoch 19/100, Loss: 0.6923818911952473, Val Accuracy: 0.5234521575984991\n",
      "Epoch 20/100, Loss: 0.6922979107063808, Val Accuracy: 0.525328330206379\n",
      "Epoch 21/100, Loss: 0.6921808545509082, Val Accuracy: 0.5262664165103189\n",
      "Epoch 22/100, Loss: 0.6920707087391771, Val Accuracy: 0.5356472795497186\n",
      "Epoch 23/100, Loss: 0.6919196464595724, Val Accuracy: 0.5168855534709194\n",
      "Epoch 24/100, Loss: 0.6917926349889919, Val Accuracy: 0.5262664165103189\n",
      "Epoch 25/100, Loss: 0.6915231619434856, Val Accuracy: 0.5422138836772983\n",
      "Epoch 26/100, Loss: 0.6913576876179556, Val Accuracy: 0.5328330206378987\n",
      "Epoch 27/100, Loss: 0.690984886237298, Val Accuracy: 0.5178236397748592\n",
      "Epoch 28/100, Loss: 0.6904082892092873, Val Accuracy: 0.5112570356472795\n",
      "Epoch 29/100, Loss: 0.6900323209691137, Val Accuracy: 0.5356472795497186\n",
      "Epoch 30/100, Loss: 0.6883555715450187, Val Accuracy: 0.5393996247654784\n",
      "Epoch 31/100, Loss: 0.6866277592458975, Val Accuracy: 0.5365853658536586\n",
      "Epoch 32/100, Loss: 0.684339688288585, Val Accuracy: 0.5215759849906192\n",
      "Epoch 33/100, Loss: 0.6825309690464748, Val Accuracy: 0.5309568480300187\n",
      "Epoch 34/100, Loss: 0.680783268887452, Val Accuracy: 0.5168855534709194\n",
      "Epoch 35/100, Loss: 0.6799425440334649, Val Accuracy: 0.5478424015009381\n",
      "Epoch 36/100, Loss: 0.6777665197626035, Val Accuracy: 0.5600375234521576\n",
      "Epoch 37/100, Loss: 0.6731236112698187, Val Accuracy: 0.5928705440900562\n",
      "Epoch 38/100, Loss: 0.6725517739070935, Val Accuracy: 0.5365853658536586\n",
      "Epoch 39/100, Loss: 0.6704438612255711, Val Accuracy: 0.600375234521576\n",
      "Epoch 40/100, Loss: 0.6695042902164245, Val Accuracy: 0.5112570356472795\n",
      "Epoch 41/100, Loss: 0.6662170761979921, Val Accuracy: 0.5881801125703565\n",
      "Epoch 42/100, Loss: 0.666482207257203, Val Accuracy: 0.5469043151969981\n",
      "Epoch 43/100, Loss: 0.6620769543147712, Val Accuracy: 0.5619136960600375\n",
      "Epoch 44/100, Loss: 0.662344698825579, Val Accuracy: 0.5834896810506567\n",
      "Epoch 45/100, Loss: 0.6615490531653501, Val Accuracy: 0.5590994371482176\n",
      "Epoch 46/100, Loss: 0.6599625453966834, Val Accuracy: 0.5450281425891182\n",
      "Epoch 47/100, Loss: 0.6549872675638521, Val Accuracy: 0.6219512195121951\n",
      "Epoch 48/100, Loss: 0.6563361034857647, Val Accuracy: 0.574108818011257\n",
      "Epoch 49/100, Loss: 0.6582886221703519, Val Accuracy: 0.575984990619137\n",
      "Epoch 50/100, Loss: 0.6571284693278624, Val Accuracy: 0.6313320825515948\n",
      "Epoch 51/100, Loss: 0.6510654395439205, Val Accuracy: 0.5975609756097561\n",
      "Epoch 52/100, Loss: 0.6525459666823626, Val Accuracy: 0.6181988742964353\n",
      "Epoch 53/100, Loss: 0.6486465015661403, Val Accuracy: 0.6341463414634146\n",
      "Epoch 54/100, Loss: 0.6491755051782515, Val Accuracy: 0.5797373358348968\n",
      "Epoch 55/100, Loss: 0.6477788860878247, Val Accuracy: 0.6200750469043153\n",
      "Epoch 56/100, Loss: 0.6474023104160466, Val Accuracy: 0.6369606003752345\n",
      "Epoch 57/100, Loss: 0.6438847152927841, Val Accuracy: 0.6303939962476548\n",
      "Epoch 58/100, Loss: 0.6441685296176525, Val Accuracy: 0.6172607879924953\n",
      "Epoch 59/100, Loss: 0.6409350533163949, Val Accuracy: 0.5112570356472795\n",
      "Epoch 60/100, Loss: 0.6422759091362971, Val Accuracy: 0.575984990619137\n",
      "Epoch 61/100, Loss: 0.6427447189775746, Val Accuracy: 0.6303939962476548\n",
      "Epoch 62/100, Loss: 0.6386010263966264, Val Accuracy: 0.6332082551594747\n",
      "Epoch 63/100, Loss: 0.640409200937114, Val Accuracy: 0.6472795497185742\n",
      "Epoch 64/100, Loss: 0.6385532421119204, Val Accuracy: 0.5450281425891182\n",
      "Epoch 65/100, Loss: 0.6364777679746964, Val Accuracy: 0.648217636022514\n",
      "Epoch 66/100, Loss: 0.6356214885408066, Val Accuracy: 0.6435272045028143\n",
      "Epoch 67/100, Loss: 0.6326536962602023, Val Accuracy: 0.5778611632270169\n",
      "Epoch 68/100, Loss: 0.6351250771726116, Val Accuracy: 0.6294559099437148\n",
      "Epoch 69/100, Loss: 0.632418437508608, Val Accuracy: 0.6181988742964353\n",
      "Epoch 70/100, Loss: 0.6340310004320038, Val Accuracy: 0.649155722326454\n",
      "Epoch 71/100, Loss: 0.6269873798116763, Val Accuracy: 0.6322701688555347\n",
      "Epoch 72/100, Loss: 0.6340155155024725, Val Accuracy: 0.5656660412757973\n",
      "Epoch 73/100, Loss: 0.6308926381868369, Val Accuracy: 0.6135084427767354\n",
      "Epoch 74/100, Loss: 0.6288157364402371, Val Accuracy: 0.6378986866791745\n",
      "Epoch 75/100, Loss: 0.6328240271364705, Val Accuracy: 0.6200750469043153\n",
      "Epoch 76/100, Loss: 0.6325200984763742, Val Accuracy: 0.600375234521576\n",
      "Epoch 77/100, Loss: 0.6321733520272073, Val Accuracy: 0.525328330206379\n",
      "Epoch 78/100, Loss: 0.626908315589812, Val Accuracy: 0.575984990619137\n",
      "Epoch 79/100, Loss: 0.6273835900794255, Val Accuracy: 0.6463414634146342\n",
      "Epoch 80/100, Loss: 0.6264733418766479, Val Accuracy: 0.574108818011257\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 32 Optimizer: sgd, Validation Accuracy: 0.649155722326454\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 32, Hidden Size: 123, Optimizer: rmsprop\n",
      "Epoch 1/100, Loss: 0.6941316138939018, Val Accuracy: 0.5\n",
      "Epoch 2/100, Loss: 0.6918742089682304, Val Accuracy: 0.5300187617260788\n",
      "Epoch 3/100, Loss: 0.6909065067991336, Val Accuracy: 0.5290806754221389\n",
      "Epoch 4/100, Loss: 0.6902546201752366, Val Accuracy: 0.5178236397748592\n",
      "Epoch 5/100, Loss: 0.6871025819010502, Val Accuracy: 0.5328330206378987\n",
      "Epoch 6/100, Loss: 0.6830530164393593, Val Accuracy: 0.5487804878048781\n",
      "Epoch 7/100, Loss: 0.6950399096538958, Val Accuracy: 0.5365853658536586\n",
      "Epoch 8/100, Loss: 0.6932330656140931, Val Accuracy: 0.5300187617260788\n",
      "Epoch 9/100, Loss: 0.688505370518688, Val Accuracy: 0.5347091932457786\n",
      "Epoch 10/100, Loss: 0.6786594953429833, Val Accuracy: 0.5309568480300187\n",
      "Epoch 11/100, Loss: 0.674314712986964, Val Accuracy: 0.5422138836772983\n",
      "Epoch 12/100, Loss: 0.6743219787261906, Val Accuracy: 0.5431519699812383\n",
      "Epoch 13/100, Loss: 0.6781030857607666, Val Accuracy: 0.549718574108818\n",
      "Epoch 14/100, Loss: 0.6713105120908901, Val Accuracy: 0.5356472795497186\n",
      "Epoch 15/100, Loss: 0.6674025884728306, Val Accuracy: 0.5337711069418386\n",
      "Epoch 16/100, Loss: 0.6683865711930093, Val Accuracy: 0.5356472795497186\n",
      "Epoch 17/100, Loss: 0.6687178464418047, Val Accuracy: 0.5328330206378987\n",
      "Epoch 18/100, Loss: 0.6696222884378183, Val Accuracy: 0.5469043151969981\n",
      "Epoch 19/100, Loss: 0.6680444240123592, Val Accuracy: 0.5318949343339587\n",
      "Epoch 20/100, Loss: 0.67236393176661, Val Accuracy: 0.5262664165103189\n",
      "Epoch 21/100, Loss: 0.6749785270137287, Val Accuracy: 0.5356472795497186\n",
      "Epoch 22/100, Loss: 0.6734623766124025, Val Accuracy: 0.5375234521575984\n",
      "Epoch 23/100, Loss: 0.6638572307561668, Val Accuracy: 0.5562851782363978\n",
      "Epoch 24/100, Loss: 0.662472206108579, Val Accuracy: 0.5609756097560976\n",
      "Epoch 25/100, Loss: 0.6611413966850395, Val Accuracy: 0.5581613508442776\n",
      "Epoch 26/100, Loss: 0.6624989384569032, Val Accuracy: 0.5666041275797373\n",
      "Epoch 27/100, Loss: 0.6633456813708674, Val Accuracy: 0.5637898686679175\n",
      "Epoch 28/100, Loss: 0.6645034288199206, Val Accuracy: 0.5422138836772983\n",
      "Epoch 29/100, Loss: 0.6621992320157168, Val Accuracy: 0.5628517823639775\n",
      "Epoch 30/100, Loss: 0.6607696539007323, Val Accuracy: 0.5356472795497186\n",
      "Epoch 31/100, Loss: 0.6618903073925204, Val Accuracy: 0.5459662288930581\n",
      "Epoch 32/100, Loss: 0.6654050510920836, Val Accuracy: 0.5628517823639775\n",
      "Epoch 33/100, Loss: 0.6641285361422136, Val Accuracy: 0.551594746716698\n",
      "Epoch 34/100, Loss: 0.6634431034884649, Val Accuracy: 0.5637898686679175\n",
      "Epoch 35/100, Loss: 0.6701918056841647, Val Accuracy: 0.5440900562851783\n",
      "Epoch 36/100, Loss: 0.6677950993459323, Val Accuracy: 0.5637898686679175\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 32 Optimizer: rmsprop, Validation Accuracy: 0.5666041275797373\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 64, Hidden Size: 123, Optimizer: adam\n",
      "Epoch 1/100, Loss: 0.6954100412219318, Val Accuracy: 0.5178236397748592\n",
      "Epoch 2/100, Loss: 0.6887186725637806, Val Accuracy: 0.5375234521575984\n",
      "Epoch 3/100, Loss: 0.6857926640937577, Val Accuracy: 0.5422138836772983\n",
      "Epoch 4/100, Loss: 0.6809387242616113, Val Accuracy: 0.5562851782363978\n",
      "Epoch 5/100, Loss: 0.6758069333745472, Val Accuracy: 0.5469043151969981\n",
      "Epoch 6/100, Loss: 0.6738514117340544, Val Accuracy: 0.5600375234521576\n",
      "Epoch 7/100, Loss: 0.6727076586502702, Val Accuracy: 0.5666041275797373\n",
      "Epoch 8/100, Loss: 0.6697666413748442, Val Accuracy: 0.5450281425891182\n",
      "Epoch 9/100, Loss: 0.6665907279768987, Val Accuracy: 0.5478424015009381\n",
      "Epoch 10/100, Loss: 0.6643256754127901, Val Accuracy: 0.5609756097560976\n",
      "Epoch 11/100, Loss: 0.6618006611937908, Val Accuracy: 0.549718574108818\n",
      "Epoch 12/100, Loss: 0.6611018381012019, Val Accuracy: 0.5572232645403377\n",
      "Epoch 13/100, Loss: 0.659168178465829, Val Accuracy: 0.5581613508442776\n",
      "Epoch 14/100, Loss: 0.6540888619067063, Val Accuracy: 0.5328330206378987\n",
      "Epoch 15/100, Loss: 0.6565190117750594, Val Accuracy: 0.5544090056285178\n",
      "Epoch 16/100, Loss: 0.6576276652848543, Val Accuracy: 0.5469043151969981\n",
      "Epoch 17/100, Loss: 0.6535833691483113, Val Accuracy: 0.5309568480300187\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 64 Optimizer: adam, Validation Accuracy: 0.5666041275797373\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 64, Hidden Size: 123, Optimizer: sgd\n",
      "Epoch 1/100, Loss: 0.6932502771491436, Val Accuracy: 0.5065666041275797\n",
      "Epoch 2/100, Loss: 0.6929117948261659, Val Accuracy: 0.49906191369606\n",
      "Epoch 3/100, Loss: 0.6930009504752372, Val Accuracy: 0.5121951219512195\n",
      "Epoch 4/100, Loss: 0.6929194904975037, Val Accuracy: 0.5028142589118199\n",
      "Epoch 5/100, Loss: 0.6929179901507363, Val Accuracy: 0.4981238273921201\n",
      "Epoch 6/100, Loss: 0.6928459187941765, Val Accuracy: 0.5093808630393997\n",
      "Epoch 7/100, Loss: 0.6928606073358166, Val Accuracy: 0.4971857410881801\n",
      "Epoch 8/100, Loss: 0.6928142157953177, Val Accuracy: 0.5037523452157598\n",
      "Epoch 9/100, Loss: 0.69270326842123, Val Accuracy: 0.5056285178236398\n",
      "Epoch 10/100, Loss: 0.6927228209687702, Val Accuracy: 0.5103189493433395\n",
      "Epoch 11/100, Loss: 0.6927376322781862, Val Accuracy: 0.5150093808630394\n",
      "Epoch 12/100, Loss: 0.692692340754751, Val Accuracy: 0.5140712945590994\n",
      "Epoch 13/100, Loss: 0.6926860698123476, Val Accuracy: 0.5103189493433395\n",
      "Epoch 14/100, Loss: 0.6925883306496179, Val Accuracy: 0.5046904315196998\n",
      "Epoch 15/100, Loss: 0.6925750316968605, Val Accuracy: 0.5093808630393997\n",
      "Epoch 16/100, Loss: 0.6925624306522199, Val Accuracy: 0.5225140712945591\n",
      "Epoch 17/100, Loss: 0.6925755503462322, Val Accuracy: 0.5065666041275797\n",
      "Epoch 18/100, Loss: 0.6925005396800255, Val Accuracy: 0.525328330206379\n",
      "Epoch 19/100, Loss: 0.6924376158571955, Val Accuracy: 0.5178236397748592\n",
      "Epoch 20/100, Loss: 0.6924653168934495, Val Accuracy: 0.5140712945590994\n",
      "Epoch 21/100, Loss: 0.6924080123652273, Val Accuracy: 0.5168855534709194\n",
      "Epoch 22/100, Loss: 0.6924094686757273, Val Accuracy: 0.5187617260787992\n",
      "Epoch 23/100, Loss: 0.6923449840118636, Val Accuracy: 0.5131332082551595\n",
      "Epoch 24/100, Loss: 0.6923022559329645, Val Accuracy: 0.5056285178236398\n",
      "Epoch 25/100, Loss: 0.6921184307603694, Val Accuracy: 0.5056285178236398\n",
      "Epoch 26/100, Loss: 0.6921864823618932, Val Accuracy: 0.5159474671669794\n",
      "Epoch 27/100, Loss: 0.6921424447600522, Val Accuracy: 0.5196998123827392\n",
      "Epoch 28/100, Loss: 0.6920740684466575, Val Accuracy: 0.5121951219512195\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 64 Optimizer: sgd, Validation Accuracy: 0.525328330206379\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 64, Hidden Size: 123, Optimizer: rmsprop\n",
      "Epoch 1/100, Loss: 0.7139202334097962, Val Accuracy: 0.5206378986866792\n",
      "Epoch 2/100, Loss: 0.6913340594341506, Val Accuracy: 0.4915572232645403\n",
      "Epoch 3/100, Loss: 0.6900123313291749, Val Accuracy: 0.4981238273921201\n",
      "Epoch 4/100, Loss: 0.6910735822435635, Val Accuracy: 0.5065666041275797\n",
      "Epoch 5/100, Loss: 0.6896540349099174, Val Accuracy: 0.49437148217636023\n",
      "Epoch 6/100, Loss: 0.6906645409206846, Val Accuracy: 0.49906191369606\n",
      "Epoch 7/100, Loss: 0.6896338049155563, Val Accuracy: 0.5290806754221389\n",
      "Epoch 8/100, Loss: 0.6896637513566373, Val Accuracy: 0.5206378986866792\n",
      "Epoch 9/100, Loss: 0.6889977704233198, Val Accuracy: 0.5290806754221389\n",
      "Epoch 10/100, Loss: 0.8056313826966641, Val Accuracy: 0.5225140712945591\n",
      "Epoch 11/100, Loss: 0.7385906612695153, Val Accuracy: 0.5225140712945591\n",
      "Epoch 12/100, Loss: 0.7362920321635346, Val Accuracy: 0.4774859287054409\n",
      "Epoch 13/100, Loss: 0.7238376892324704, Val Accuracy: 0.5196998123827392\n",
      "Epoch 14/100, Loss: 0.7300093116155312, Val Accuracy: 0.5225140712945591\n",
      "Epoch 15/100, Loss: 0.7186657145841798, Val Accuracy: 0.5281425891181989\n",
      "Epoch 16/100, Loss: 0.7252590820860507, Val Accuracy: 0.5196998123827392\n",
      "Epoch 17/100, Loss: 0.7083590030670166, Val Accuracy: 0.4774859287054409\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 64 Optimizer: rmsprop, Validation Accuracy: 0.5290806754221389\n",
      "Best Model Configuration: {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'adam'} with Validation Accuracy: 0.6848030018761726 over 24 epochs\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01]\n",
    "batch_sizes = [32, 64]\n",
    "hidden_size = 123\n",
    "optimizers = ['adam', 'sgd', 'rmsprop']  \n",
    "\n",
    "embedding_matrix_list = [embedding_matrix[word] for word in vocab]\n",
    "embedding_matrix_array = np.stack(embedding_matrix_list)\n",
    "vocab_size, embedding_dim = embedding_matrix_array.shape\n",
    "output_size = 1  \n",
    "\n",
    "\n",
    "best_val_acc = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "            for opt in optimizers:\n",
    "                print(\"Training with the following hyperparameters:\")\n",
    "                print(f\"Learning Rate: {lr}, Batch Size: {bs}, Hidden Size: {hidden_size}, Optimizer: {opt}\")\n",
    "                # Initialize model, criterion\n",
    "                model = RNNModel(embedding_matrix_array, hidden_size=128, output_size=1)\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "                # Initialize optimizer based on the selected type\n",
    "                if opt == 'adam':\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                elif opt == 'sgd':\n",
    "                    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "                elif opt == 'rmsprop':\n",
    "                    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "                train_loader = create_data_loader(train_dataset_instance, bs)\n",
    "                val_loader = create_data_loader(val_dataset_instance, bs)\n",
    "                \n",
    "                # Train and validate\n",
    "                val_acc, epochs_used = train_and_validate(model, train_loader, val_loader, optimizer, criterion)\n",
    "                print(f\"Learning Rate: {lr}, Batch Size: {bs} Optimizer: {opt}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "                # Update best parameters\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_hyperparams = {\n",
    "                        'learning_rate': lr,\n",
    "                        'batch_size': bs,\n",
    "                        'optimizer': opt\n",
    "                    }\n",
    "                    best_epochs = epochs_used\n",
    "\n",
    "# Print the best configuration\n",
    "print(f\"Best Model Configuration: {best_hyperparams} with Validation Accuracy: {best_val_acc} over {best_epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6932030738289676, Val Accuracy: 0.575046904315197\n",
      "Epoch 2/100, Loss: 0.6585904124067791, Val Accuracy: 0.6313320825515948\n",
      "Epoch 3/100, Loss: 0.6413114764797154, Val Accuracy: 0.6069418386491557\n",
      "Epoch 4/100, Loss: 0.6170370576541815, Val Accuracy: 0.6144465290806754\n",
      "Epoch 5/100, Loss: 0.6095510436527765, Val Accuracy: 0.6538461538461539\n",
      "Epoch 6/100, Loss: 0.6074308230360942, Val Accuracy: 0.6594746716697936\n",
      "Epoch 7/100, Loss: 0.5941777089193686, Val Accuracy: 0.6632270168855535\n",
      "Epoch 8/100, Loss: 0.5925311423949341, Val Accuracy: 0.6594746716697936\n",
      "Epoch 9/100, Loss: 0.5892993661005106, Val Accuracy: 0.650093808630394\n",
      "Epoch 10/100, Loss: 0.5830642238481721, Val Accuracy: 0.6538461538461539\n",
      "Epoch 11/100, Loss: 0.584717085334792, Val Accuracy: 0.6435272045028143\n",
      "Epoch 12/100, Loss: 0.5767381233510687, Val Accuracy: 0.6697936210131332\n",
      "Epoch 13/100, Loss: 0.5795326408610415, Val Accuracy: 0.6707317073170732\n",
      "Epoch 14/100, Loss: 0.5749036865447884, Val Accuracy: 0.6641651031894934\n",
      "Epoch 15/100, Loss: 0.5774902147588445, Val Accuracy: 0.6669793621013134\n",
      "Epoch 16/100, Loss: 0.565848623416317, Val Accuracy: 0.6651031894934334\n",
      "Epoch 17/100, Loss: 0.5740577253832746, Val Accuracy: 0.6819887429643527\n",
      "Epoch 18/100, Loss: 0.5706073746307573, Val Accuracy: 0.6707317073170732\n",
      "Epoch 19/100, Loss: 0.5635841044027414, Val Accuracy: 0.6575984990619137\n",
      "Epoch 20/100, Loss: 0.5704263462059533, Val Accuracy: 0.6303939962476548\n",
      "Epoch 21/100, Loss: 0.5681544951538542, Val Accuracy: 0.6632270168855535\n",
      "Epoch 22/100, Loss: 0.5627726474359854, Val Accuracy: 0.6378986866791745\n",
      "Epoch 23/100, Loss: 0.5646430038249315, Val Accuracy: 0.6566604127579737\n",
      "Epoch 24/100, Loss: 0.5611289701799849, Val Accuracy: 0.6651031894934334\n",
      "Epoch 25/100, Loss: 0.5533122146752343, Val Accuracy: 0.6688555347091932\n",
      "Epoch 26/100, Loss: 0.5572246508811837, Val Accuracy: 0.6547842401500938\n",
      "Epoch 27/100, Loss: 0.5708707755626138, Val Accuracy: 0.6538461538461539\n",
      "Convergence reached, stopping training.\n",
      "Validation Accuracy: 0.6819887429643527, over 26 epochs\n"
     ]
    }
   ],
   "source": [
    "#Trainig the model with the best hyperparameters\n",
    "batch_size = 64\n",
    "lr=0.001\n",
    "model = RNNModel(embedding_matrix_array, hidden_size=128, output_size=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_loader = create_data_loader(train_dataset_instance, batch_size)\n",
    "val_loader = create_data_loader(val_dataset_instance, batch_size)\n",
    "                \n",
    "# Train and validate\n",
    "val_acc, epochs_used = train_and_validate(model, train_loader, val_loader, optimizer, criterion)\n",
    "print(f\"Validation Accuracy: {val_acc}, over {epochs_used} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6472795497185742\n",
      "Final Configuration:\n",
      "Epochs: 26\n",
      "Learning Rate: 0.001\n",
      "Optimizer: Adam\n",
      "Batch Size: 64\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate on Test Set\n",
    "test_loader = create_data_loader(test_dataset_instance, batch_size)\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Report the configuration\n",
    "print(f\"Final Configuration:\\nEpochs: {epochs_used}\\nLearning Rate: {lr}\\nOptimizer: Adam\\nBatch Size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Select a random index from the test dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m random_index \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_dataset\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get the corresponding sentence and its label from the test dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m sample_sentence \u001b[38;5;241m=\u001b[39m test_dataset[random_index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Assuming the dataset contains a 'text' field\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 8: Get a sample sentence from the test set and predict\n",
    "import random\n",
    "# Select a random index from the test dataset\n",
    "random_index = random.randint(0, len(test_dataset) - 1)\n",
    "\n",
    "# Get the corresponding sentence and its label from the test dataset\n",
    "sample_sentence = test_dataset[random_index]['text']  # Assuming the dataset contains a 'text' field\n",
    "true_label = test_dataset[random_index]['label']  # Assuming there's a label field\n",
    "\n",
    "# Tokenize the sample sentence\n",
    "sample_tokens = word_tokenize(sample_sentence.lower())\n",
    "\n",
    "# Convert tokens to indices\n",
    "sample_indices = []\n",
    "for token in sample_tokens:\n",
    "    if token in vocab:\n",
    "        sample_indices.append(list(vocab).index(token))\n",
    "    else:\n",
    "        sample_indices.append(list(vocab).index(\"<UNK>\"))\n",
    "sample_tensor = torch.tensor(sample_indices).unsqueeze(0)  # Add batch dimension\n",
    "# Make prediction using the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to compute gradients during inference\n",
    "    output = model(sample_tensor)  # Pass the tensor to the model\n",
    "    _, predicted = torch.max(output, 1)  # Get the index of the max log-probability\n",
    "\n",
    "# Map predicted index to sentiment label\n",
    "sentiment_labels = ['negative', 'positive']  # Adjust according to your label encoding\n",
    "predicted_label = sentiment_labels[predicted.item()]\n",
    "\n",
    "# Print results\n",
    "print(f\"Sample Sentence: '{sample_sentence}'\")\n",
    "print(f\"True Label: {true_label}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
