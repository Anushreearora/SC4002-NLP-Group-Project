{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8530 sentences\n",
      "Size of validation set: 1066 sentences\n",
      "Size of test set: 1066 sentences\n"
     ]
    }
   ],
   "source": [
    "#Number of sentences in each set \n",
    "print(f\"Size of training set: {train_dataset.num_rows} sentences\")\n",
    "print(f\"Size of validation set: {validation_dataset.num_rows} sentences\")\n",
    "print(f\"Size of test set: {test_dataset.num_rows} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding matrix with shape: (16633, 300)\n",
      "Vocabulary size (word_to_index): 16633\n"
     ]
    }
   ],
   "source": [
    "# # Load spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Load the embedding matrix and word_to_index from the pickle file\n",
    "# with open(\"updated_embedding_matrix.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "#     embedding_matrix = data[\"embeddings\"]\n",
    "#     word_to_index = data[\"word_to_index\"]\n",
    "\n",
    "# # Convert embedding_matrix to a NumPy array and a PyTorch tensor\n",
    "# embedding_matrix_array = np.array(embedding_matrix)\n",
    "# embedding_matrix_tensor = torch.tensor(embedding_matrix_array, dtype=torch.float32)\n",
    "\n",
    "# print(f\"Loaded embedding matrix with shape: {embedding_matrix_array.shape}\")\n",
    "# print(f\"Vocabulary size (word_to_index): {len(word_to_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenise train, validation, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_tokenized_train_texts = []\n",
    "# for sentence in train_dataset['text']:\n",
    "#     # Tokenize the sentence using spaCy and store tokens as a list of strings\n",
    "#     tokens = [token.text for token in nlp(sentence.lower())]\n",
    "#     pre_tokenized_train_texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pre-tokenize validation and test sets\n",
    "# pre_tokenized_validation_texts = [[token.text for token in nlp(sentence.lower())] for sentence in validation_dataset['text']]\n",
    "# pre_tokenized_test_texts = [[token.text for token in nlp(sentence.lower())] for sentence in test_dataset['text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3.Keeping the above two adjustments, replace your simple RNN model in Part 2 wioth a biLSTM model and biGRU model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# biLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing the train dataset. text -> word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures reproducibility in CUDA operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disables some optimizations to ensure determinism\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels : list[int], vocab : set, embedding_matrix : dict, max_len=30):\n",
    "        self.texts = tokenized_texts\n",
    "        self.labels  = labels\n",
    "        self.vocab = word_to_index\n",
    "        self.embedding_matrix : dict = embedding_matrix\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        vectorized_text = self.vectorize(text)\n",
    "        return torch.tensor(vectorized_text, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "            \n",
    "    def vectorize(self, tokens):\n",
    "        # Convert tokens to their corresponding index in the vocabulary\n",
    "        vectorized = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "        # Pad or truncate to max_len\n",
    "        if len(vectorized) < self.max_len:\n",
    "            vectorized += [self.vocab['<PAD>']] * (self.max_len - len(vectorized))\n",
    "        else:\n",
    "            vectorized = vectorized[:self.max_len]\n",
    "        return vectorized\n",
    "\n",
    "    def build_vocab_dict(self, vocab : set):\n",
    "        if \"<PAD>\" in vocab : vocab.remove(\"<PAD>\")\n",
    "        if \"<UNK>\" in vocab : vocab.remove(\"<UNK>\")\n",
    "        vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "        vocab_dict['<PAD>'] = len(vocab_dict) # Add padding token\n",
    "        vocab_dict['<UNK>'] = len(vocab_dict) # Add unknown token\n",
    "        return vocab_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentBiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SentimentBiLSTM, self).__init__()\n",
    "        # Load pre-trained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size(1), hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = torch.cat((lstm_out[:, -1, :self.lstm.hidden_size], lstm_out[:, 0, self.lstm.hidden_size:]), dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in iterator:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch).squeeze(1)\n",
    "        loss = criterion(output, y_batch.float())\n",
    "        loss.backward()\n",
    "        # for param in model.parameters():\n",
    "        #     if param.grad is not None:  # Ensure the gradient is not None\n",
    "        #         print(f\"Gradient norm for {param.shape}: {param.grad.data.norm()}\")\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in iterator:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "            output = model(X_batch).squeeze(1)\n",
    "            \n",
    "            probs = model.sigmoid(output)\n",
    "            prediction = (probs >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(output, y_batch.float())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(prediction.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy, epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(num_epochs, model, train_iterator, valid_iterator, optimizer, criterion, scheduler):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "        accuracy , valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        if (epoch==0):\n",
    "                best_acc = accuracy\n",
    "                epochs_without_improvement = 0\n",
    "        print(f'Epoch {epoch + 1}: Train Loss = {train_loss:.3f}, Accuracy = {accuracy:.3f}, Val Loss = {valid_loss:.3f} Learning Rate: {scheduler.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Check for convergence\n",
    "        if epochs_without_improvement >= 6:  # Convergence condition (no improvement for 4 epochs)\n",
    "            print(\"Convergence reached, stopping training.\")\n",
    "            break\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model and run the train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding_matrix : dict[ str , np.ndarray]= np.load(\"embedding_matrix.npy\",allow_pickle='TRUE').item()\n",
    "# embedding_matrix_values = np.array(list(embedding_matrix.values()), dtype=np.float32)\n",
    "# embedding_matrix_tensor = torch.tensor(embedding_matrix_values, dtype=torch.float32)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = SentimentDataset(pre_tokenized_train_texts, train_dataset['label'], word_to_index, embedding_matrix)\n",
    "valid_dataset = SentimentDataset(pre_tokenized_validation_texts, validation_dataset['label'], word_to_index, embedding_matrix)\n",
    "test_dataset = SentimentDataset(pre_tokenized_test_texts, test_dataset['label'], word_to_index, embedding_matrix)\n",
    "# Create data loaders\n",
    "train_iterator = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128  # Adjust as needed\n",
    "output_dim = 1  # Binary sentiment classification\n",
    "model = SentimentBiLSTM(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate=0.0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.559, Accuracy = 0.743, Val Loss = 0.524 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.353, Accuracy = 0.778, Val Loss = 0.475 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.177, Accuracy = 0.767, Val Loss = 0.644 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.071, Accuracy = 0.768, Val Loss = 0.768 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.027, Accuracy = 0.770, Val Loss = 0.916 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.016, Accuracy = 0.772, Val Loss = 0.955 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.006, Accuracy = 0.759, Val Loss = 1.035 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.003, Accuracy = 0.771, Val Loss = 1.185 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.001, Accuracy = 0.773, Val Loss = 1.252 Learning Rate: 0.000100\n",
      "Epoch 10: Train Loss = 0.001, Accuracy = 0.772, Val Loss = 1.313 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
    "# Now you can run your training loop\n",
    "train_and_validate(25, model, train_iterator, valid_iterator, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.3, batch_size=32\n",
      "Epoch 1: Train Loss = 0.581, Accuracy = 0.775, Val Loss = 0.483 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.358, Accuracy = 0.778, Val Loss = 0.494 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.180, Accuracy = 0.771, Val Loss = 0.547 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.075, Accuracy = 0.760, Val Loss = 0.785 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.034, Accuracy = 0.761, Val Loss = 0.958 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.020, Accuracy = 0.762, Val Loss = 1.000 Learning Rate: 0.000100\n",
      "Epoch 7: Train Loss = 0.007, Accuracy = 0.757, Val Loss = 1.102 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.005, Accuracy = 0.758, Val Loss = 1.154 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.3, batch_size=64\n",
      "Epoch 1: Train Loss = 0.605, Accuracy = 0.735, Val Loss = 0.533 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.395, Accuracy = 0.749, Val Loss = 0.561 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.260, Accuracy = 0.763, Val Loss = 0.645 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.149, Accuracy = 0.765, Val Loss = 0.682 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.070, Accuracy = 0.750, Val Loss = 0.742 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.036, Accuracy = 0.753, Val Loss = 0.909 Learning Rate: 0.000100\n",
      "Epoch 7: Train Loss = 0.017, Accuracy = 0.752, Val Loss = 1.029 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.014, Accuracy = 0.747, Val Loss = 1.067 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.012, Accuracy = 0.745, Val Loss = 1.108 Learning Rate: 0.000100\n",
      "Epoch 10: Train Loss = 0.011, Accuracy = 0.742, Val Loss = 1.145 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.5, batch_size=32\n",
      "Epoch 1: Train Loss = 0.575, Accuracy = 0.760, Val Loss = 0.507 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.364, Accuracy = 0.776, Val Loss = 0.459 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.197, Accuracy = 0.765, Val Loss = 0.575 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.093, Accuracy = 0.765, Val Loss = 0.672 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.050, Accuracy = 0.760, Val Loss = 0.749 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.027, Accuracy = 0.752, Val Loss = 1.025 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.013, Accuracy = 0.761, Val Loss = 1.093 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.007, Accuracy = 0.752, Val Loss = 1.194 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.5, batch_size=64\n",
      "Epoch 1: Train Loss = 0.613, Accuracy = 0.720, Val Loss = 0.529 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.404, Accuracy = 0.785, Val Loss = 0.475 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.254, Accuracy = 0.781, Val Loss = 0.510 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.138, Accuracy = 0.758, Val Loss = 0.625 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.065, Accuracy = 0.754, Val Loss = 0.800 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.035, Accuracy = 0.763, Val Loss = 0.993 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.022, Accuracy = 0.774, Val Loss = 1.083 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.013, Accuracy = 0.758, Val Loss = 1.115 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.3, batch_size=32\n",
      "Epoch 1: Train Loss = 0.565, Accuracy = 0.756, Val Loss = 0.485 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.342, Accuracy = 0.775, Val Loss = 0.481 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.170, Accuracy = 0.766, Val Loss = 0.556 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.079, Accuracy = 0.782, Val Loss = 0.709 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.037, Accuracy = 0.774, Val Loss = 0.927 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.018, Accuracy = 0.766, Val Loss = 1.110 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.009, Accuracy = 0.762, Val Loss = 1.174 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.003, Accuracy = 0.766, Val Loss = 1.270 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.002, Accuracy = 0.762, Val Loss = 1.345 Learning Rate: 0.000100\n",
      "Epoch 10: Train Loss = 0.002, Accuracy = 0.756, Val Loss = 1.380 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.3, batch_size=64\n",
      "Epoch 1: Train Loss = 0.585, Accuracy = 0.738, Val Loss = 0.518 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.383, Accuracy = 0.768, Val Loss = 0.481 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.230, Accuracy = 0.775, Val Loss = 0.527 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.121, Accuracy = 0.768, Val Loss = 0.594 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.060, Accuracy = 0.748, Val Loss = 0.885 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.029, Accuracy = 0.766, Val Loss = 0.994 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.013, Accuracy = 0.759, Val Loss = 1.186 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.005, Accuracy = 0.757, Val Loss = 1.227 Learning Rate: 0.000100\n",
      "Epoch 9: Train Loss = 0.004, Accuracy = 0.763, Val Loss = 1.262 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.5, batch_size=32\n",
      "Epoch 1: Train Loss = 0.566, Accuracy = 0.764, Val Loss = 0.492 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.353, Accuracy = 0.774, Val Loss = 0.464 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.192, Accuracy = 0.773, Val Loss = 0.543 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.086, Accuracy = 0.768, Val Loss = 0.651 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.038, Accuracy = 0.770, Val Loss = 0.855 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.020, Accuracy = 0.766, Val Loss = 1.046 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.015, Accuracy = 0.756, Val Loss = 0.973 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.006, Accuracy = 0.757, Val Loss = 1.112 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.5, batch_size=64\n",
      "Epoch 1: Train Loss = 0.577, Accuracy = 0.754, Val Loss = 0.492 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.382, Accuracy = 0.775, Val Loss = 0.471 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.237, Accuracy = 0.765, Val Loss = 0.644 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.129, Accuracy = 0.767, Val Loss = 0.593 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.063, Accuracy = 0.765, Val Loss = 0.831 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.031, Accuracy = 0.769, Val Loss = 0.973 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.023, Accuracy = 0.757, Val Loss = 1.071 Learning Rate: 0.000100\n",
      "Epoch 8: Train Loss = 0.011, Accuracy = 0.763, Val Loss = 1.069 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n",
      "Best accuracy: 0.763 with hidden_dim=128, dropout_rate=0.3, batch_size=64\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the hyper-parameter grid\n",
    "hidden_dims = [64, 128]\n",
    "learning_rates = [0.001]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "batch_sizes = [32, 64]\n",
    "output_dim = 1 \n",
    "best_acc = 0\n",
    "best_hidden_dim = 0\n",
    "best_dropout_rate = 0\n",
    "best_bs = 0\n",
    "\n",
    "# Iterate over all combinations of hyper-parameters\n",
    "for hidden_dim, lr, dropout_rate, bs in itertools.product(hidden_dims, learning_rates, dropout_rates, batch_sizes):\n",
    "    print(f'Training with hidden_dim={hidden_dim}, lr={lr}, dropout_rate={dropout_rate}, batch_size={bs}')\n",
    "    \n",
    "    model_hyper = SentimentBiLSTM(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "    criterion_hyper = nn.BCEWithLogitsLoss()\n",
    "    optimizer_hyper = optim.Adam(model_hyper.parameters(), lr=lr)\n",
    "    scheduler_hyper = optim.lr_scheduler.ReduceLROnPlateau(optimizer_hyper, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "    train_iterator_hyper = DataLoader(train_dataset, bs, shuffle=True)\n",
    "    valid_iterator_hyper = DataLoader(valid_dataset, bs, shuffle=False)\n",
    "\n",
    "    accuracy = train_and_validate(25, model_hyper, train_iterator_hyper, valid_iterator_hyper, optimizer_hyper, criterion_hyper, scheduler_hyper)\n",
    "\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_hidden_dim = hidden_dim\n",
    "        best_dropout_rate = dropout_rate\n",
    "        best_bs = bs\n",
    "\n",
    "\n",
    "print(f'Best accuracy: {best_acc:.3f} with hidden_dim={best_hidden_dim}, dropout_rate={best_dropout_rate}, batch_size={best_bs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with best parameters : \n",
    "hidden_dim=128, \n",
    "\n",
    "lr=0.001, \n",
    "\n",
    "dropout_rate=0.3, \n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.577, Accuracy = 0.726, Val Loss = 0.530 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.387, Accuracy = 0.761, Val Loss = 0.477 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.234, Accuracy = 0.768, Val Loss = 0.527 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.122, Accuracy = 0.767, Val Loss = 0.648 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.056, Accuracy = 0.760, Val Loss = 0.948 Learning Rate: 0.001000\n",
      "Epoch 6: Train Loss = 0.025, Accuracy = 0.762, Val Loss = 0.911 Learning Rate: 0.001000\n",
      "Epoch 7: Train Loss = 0.017, Accuracy = 0.759, Val Loss = 1.151 Learning Rate: 0.000100\n",
      "Convergence reached, stopping training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7589118198874296"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim_best = 128\n",
    "lr_best=0.001\n",
    "dropout_rate_best=0.3\n",
    "bs_best=64\n",
    "output_dim = 1 \n",
    "\n",
    "model_best = SentimentBiLSTM(embedding_matrix_tensor, hidden_dim_best, output_dim, dropout_rate_best).to(device)\n",
    "criterion_best = nn.BCEWithLogitsLoss()\n",
    "optimizer_best = optim.Adam(model_best.parameters(), lr_best)\n",
    "scheduler_best = optim.lr_scheduler.ReduceLROnPlateau(optimizer_best, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "train_iterator_best = DataLoader(train_dataset, bs_best, shuffle=True)\n",
    "valid_iterator_best = DataLoader(valid_dataset, bs_best, shuffle=False)\n",
    "\n",
    "train_and_validate(25, model_best, train_iterator_best, valid_iterator_best, optimizer_best, criterion_best, scheduler_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in iterator:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "            output = model(X_batch).squeeze(1)\n",
    "            \n",
    "            probs = model.sigmoid(output)\n",
    "            prediction = (probs >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(output, y_batch.float())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(prediction.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "            \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy = {accuracy:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.788\n"
     ]
    }
   ],
   "source": [
    "test(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.788\n"
     ]
    }
   ],
   "source": [
    "test(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.797\n"
     ]
    }
   ],
   "source": [
    "test(model_best, test_iterator, criterion_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM with an Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: [batch_size, hidden_dim] (last hidden state of the decoder or a step in the sequence)\n",
    "        encoder_outputs: [batch_size, seq_len, hidden_dim] (outputs from the encoder)\n",
    "        \"\"\"\n",
    "        # Compute the attention weights using the query (hidden state) and keys (encoder outputs)\n",
    "        attn_weights = torch.matmul(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        # Apply softmax to get the attention weights\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        # Compute the weighted sum of the encoder outputs (values)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout_rate):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        # Load pre-trained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size(1), hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.attn = Attention(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x) #[B, S, E]\n",
    "        \n",
    "        # Get the outputs and hidden states from the LSTM\n",
    "        lstm_out,_= self.lstm(embedded)  # lstm_out: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Use the last hidden state of the LSTM as the query for the attention mechanism\n",
    "        last_hidden = lstm_out[:, -1, :] #[B, 2 * H]\n",
    "        \n",
    "        # Apply attention to the LSTM outputs\n",
    "        context = self.attn(last_hidden, lstm_out) #[B, 2 * H], [B, S]\n",
    "        \n",
    "        # Pass the context vector through a fully connected layer\n",
    "        output = self.fc(context) #[B, output_dim]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_attn = 128\n",
    "lr_attn=0.001\n",
    "dropout_rate_attn=0.3\n",
    "bs_attn=64\n",
    "output_dim = 1 \n",
    "\n",
    "\n",
    "model_attn= BiLSTMWithAttention(embedding_matrix_tensor, hidden_dim_attn, output_dim, dropout_rate_attn ).to(device)\n",
    "criterion_attn = nn.BCEWithLogitsLoss()\n",
    "optimizer_attn = torch.optim.Adam(model_attn.parameters(), lr=1e-3)\n",
    "scheduler_attn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attn, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "train_iterator_attn = DataLoader(train_dataset, bs_attn, shuffle=True)\n",
    "valid_iterator_attn = DataLoader(valid_dataset, bs_attn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.561, Accuracy = 0.768, Val Loss = 0.496 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.377, Accuracy = 0.730, Val Loss = 0.542 Learning Rate: 0.001000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7298311444652908"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validate(2, model_attn, train_iterator_attn,valid_iterator_attn, optimizer_attn, criterion_attn, scheduler_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.728\n"
     ]
    }
   ],
   "source": [
    "test(model_attn, test_iterator, criterion_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.009, Accuracy = 0.765, Val Loss = 1.267 Learning Rate: 0.000010\n",
      "Epoch 2: Train Loss = 0.008, Accuracy = 0.768, Val Loss = 1.277 Learning Rate: 0.000010\n",
      "Accuracy = 0.784\n"
     ]
    }
   ],
   "source": [
    "train_and_validate(2, model_attn, train_iterator_attn,valid_iterator_attn, optimizer_attn, criterion_attn, scheduler_attn)\n",
    "test(model_attn, test_iterator, criterion_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.3, batch_size=32\n",
      "Epoch 1: Train Loss = 0.592, Accuracy = 0.754, Val Loss = 0.508 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.366, Accuracy = 0.765, Val Loss = 0.500 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.196, Accuracy = 0.772, Val Loss = 0.542 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.086, Accuracy = 0.748, Val Loss = 0.749 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.037, Accuracy = 0.752, Val Loss = 1.038 Learning Rate: 0.001000\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.3, batch_size=64\n",
      "Epoch 1: Train Loss = 0.595, Accuracy = 0.740, Val Loss = 0.512 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.384, Accuracy = 0.758, Val Loss = 0.493 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.231, Accuracy = 0.755, Val Loss = 0.535 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.127, Accuracy = 0.752, Val Loss = 0.689 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.058, Accuracy = 0.750, Val Loss = 0.796 Learning Rate: 0.001000\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.5, batch_size=32\n",
      "Epoch 1: Train Loss = 0.571, Accuracy = 0.735, Val Loss = 0.501 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.356, Accuracy = 0.786, Val Loss = 0.466 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.184, Accuracy = 0.772, Val Loss = 0.530 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.085, Accuracy = 0.778, Val Loss = 0.675 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.042, Accuracy = 0.757, Val Loss = 0.794 Learning Rate: 0.001000\n",
      "Training with hidden_dim=64, lr=0.001, dropout_rate=0.5, batch_size=64\n",
      "Epoch 1: Train Loss = 0.598, Accuracy = 0.747, Val Loss = 0.524 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.396, Accuracy = 0.774, Val Loss = 0.489 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.248, Accuracy = 0.765, Val Loss = 0.532 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.142, Accuracy = 0.758, Val Loss = 0.686 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.077, Accuracy = 0.756, Val Loss = 0.789 Learning Rate: 0.001000\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.3, batch_size=32\n",
      "Epoch 1: Train Loss = 0.564, Accuracy = 0.760, Val Loss = 0.486 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.353, Accuracy = 0.782, Val Loss = 0.459 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.181, Accuracy = 0.788, Val Loss = 0.586 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.076, Accuracy = 0.778, Val Loss = 0.732 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.037, Accuracy = 0.774, Val Loss = 0.855 Learning Rate: 0.001000\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.3, batch_size=64\n",
      "Epoch 1: Train Loss = 0.577, Accuracy = 0.765, Val Loss = 0.484 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.374, Accuracy = 0.772, Val Loss = 0.464 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.232, Accuracy = 0.780, Val Loss = 0.561 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.123, Accuracy = 0.778, Val Loss = 0.577 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.062, Accuracy = 0.751, Val Loss = 0.894 Learning Rate: 0.001000\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.5, batch_size=32\n",
      "Epoch 1: Train Loss = 0.564, Accuracy = 0.766, Val Loss = 0.476 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.344, Accuracy = 0.775, Val Loss = 0.476 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.180, Accuracy = 0.781, Val Loss = 0.544 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.081, Accuracy = 0.770, Val Loss = 0.737 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.036, Accuracy = 0.761, Val Loss = 1.079 Learning Rate: 0.001000\n",
      "Training with hidden_dim=128, lr=0.001, dropout_rate=0.5, batch_size=64\n",
      "Epoch 1: Train Loss = 0.586, Accuracy = 0.752, Val Loss = 0.497 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.388, Accuracy = 0.769, Val Loss = 0.470 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.234, Accuracy = 0.773, Val Loss = 0.518 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.124, Accuracy = 0.770, Val Loss = 0.645 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.067, Accuracy = 0.765, Val Loss = 0.842 Learning Rate: 0.001000\n",
      "Best accuracy: 0.774 with hidden_dim=128, dropout_rate=0.3, batch_size=32\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the hyper-parameter grid\n",
    "hidden_dims = [64, 128]\n",
    "learning_rates = [0.001]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "batch_sizes = [32, 64]\n",
    "output_dim = 1 \n",
    "best_acc_attn = 0\n",
    "best_hidden_dim_attn = 0\n",
    "best_dropout_rate_attn = 0\n",
    "best_bs_attn = 0\n",
    "\n",
    "# Iterate over all combinations of hyper-parameters\n",
    "for hidden_dim, lr, dropout_rate, bs in itertools.product(hidden_dims, learning_rates, dropout_rates, batch_sizes):\n",
    "    print(f'Training with hidden_dim={hidden_dim}, lr={lr}, dropout_rate={dropout_rate}, batch_size={bs}')\n",
    "    \n",
    "    model_attn_hyper = SentimentBiLSTM(embedding_matrix_tensor, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "    criterion_attn_hyper = nn.BCEWithLogitsLoss()\n",
    "    optimizer_attn_hyper = optim.Adam(model_attn_hyper.parameters(), lr=0.001)\n",
    "    scheduler_attn_hyper = optim.lr_scheduler.ReduceLROnPlateau(optimizer_attn_hyper, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "    train_iterator_attn_hyper = DataLoader(train_dataset, bs, shuffle=True)\n",
    "    valid_iterator_attn_hyper = DataLoader(valid_dataset, bs, shuffle=False)\n",
    "\n",
    "    accuracy = train_and_validate(5, model_attn_hyper, train_iterator_attn_hyper, valid_iterator_attn_hyper, optimizer_attn_hyper, criterion_attn_hyper, scheduler_attn_hyper)\n",
    "\n",
    "    if accuracy > best_acc_attn:\n",
    "        best_acc_attn = accuracy\n",
    "        best_hidden_dim_attn = hidden_dim\n",
    "        best_dropout_rate_attn = dropout_rate\n",
    "        best_bs_attn = bs\n",
    "\n",
    "\n",
    "print(f'Best accuracy: {best_acc_attn:.3f} with hidden_dim={best_hidden_dim_attn}, dropout_rate={best_dropout_rate_attn}, batch_size={best_bs_attn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_attn_best = 128\n",
    "lr_attn=0.001\n",
    "dropout_rate_attn_best=0.3\n",
    "bs_attn_best=32\n",
    "output_dim = 1 \n",
    "\n",
    "\n",
    "model_attn_best= BiLSTMWithAttention(embedding_matrix_tensor, hidden_dim_attn_best, output_dim, dropout_rate_attn_best ).to(device)\n",
    "criterion_attn_best = nn.BCEWithLogitsLoss()\n",
    "optimizer_attn_best = torch.optim.Adam(model_attn_best.parameters(), lr=1e-3)\n",
    "scheduler_attn_best = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attn_best, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "train_iterator_attn_best = DataLoader(train_dataset, bs_attn_best, shuffle=True)\n",
    "valid_iterator_attn_best = DataLoader(valid_dataset, bs_attn_best, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.559, Accuracy = 0.762, Val Loss = 0.479 Learning Rate: 0.001000\n",
      "Epoch 2: Train Loss = 0.346, Accuracy = 0.787, Val Loss = 0.463 Learning Rate: 0.001000\n",
      "Epoch 3: Train Loss = 0.191, Accuracy = 0.782, Val Loss = 0.586 Learning Rate: 0.001000\n",
      "Epoch 4: Train Loss = 0.097, Accuracy = 0.778, Val Loss = 0.685 Learning Rate: 0.001000\n",
      "Epoch 5: Train Loss = 0.050, Accuracy = 0.783, Val Loss = 1.036 Learning Rate: 0.001000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7833020637898687"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validate(5, model_attn_best, train_iterator_attn_best, valid_iterator_attn_best, optimizer_attn_best, criterion_attn_best, scheduler_attn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.791\n"
     ]
    }
   ],
   "source": [
    "test(model_attn_best, test_iterator, criterion_attn_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
