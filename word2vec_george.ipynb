{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset_33(Dataset):\n",
    "    def __init__(self, texts : list[str], labels : list[int], vocab : set, embedding_matrix : dict, max_len=30):\n",
    "        self.texts : list[str] = texts\n",
    "        self.labels : list[int] = labels\n",
    "        self.vocab : dict = self.build_vocab_dict(vocab)  # function to build vocabulary\n",
    "        self.embedding_matrix : dict = embedding_matrix\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = word_tokenize(text.lower())\n",
    "        vectorized_text = self.vectorize(tokenized_text)\n",
    "        return torch.tensor(vectorized_text, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        # Convert tokens to their corresponding index in the vocabulary\n",
    "        vectorized = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "        # Pad or truncate to max_len\n",
    "        if len(vectorized) < self.max_len:\n",
    "            vectorized += [self.vocab['<PAD>']] * (self.max_len - len(vectorized))\n",
    "        else:\n",
    "            vectorized = vectorized[:self.max_len]\n",
    "        return vectorized\n",
    "\n",
    "    def build_vocab_dict(self, vocab : set):\n",
    "        if \"<PAD>\" in vocab : vocab.remove(\"<PAD>\")\n",
    "        if \"<UNK>\" in vocab : vocab.remove(\"<UNK>\")\n",
    "        vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "        print(len(vocab_dict))\n",
    "        vocab_dict['<PAD>'] = len(vocab_dict) # Add padding token\n",
    "        print(len(vocab_dict))\n",
    "        vocab_dict['<UNK>'] = len(vocab_dict) # Add unknown token\n",
    "        print(len(vocab_dict))\n",
    "        return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary(including padding and unknown tokens): 18031\n",
      "Number of words in the vocabulary: 18029\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = []\n",
    "for sentence in train_dataset['text']:\n",
    "    train_tokenized.append(word_tokenize(sentence.lower()))\n",
    "vocab = {\"<PAD>\", \"<UNK>\"} #include a padding and unknown token for future processing\n",
    "vocab.update(word for sentence in train_tokenized for word in sentence)\n",
    "\n",
    "print(\"Number of words in the vocabulary(including padding and unknown tokens):\", len(vocab))\n",
    "print(\"Number of words in the vocabulary:\" , len(vocab)-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18029\n",
      "18030\n",
      "18031\n",
      "18029\n",
      "18030\n",
      "18031\n"
     ]
    }
   ],
   "source": [
    "# Prepare your datasets\n",
    "train_texts_33 : list[str] = train_dataset['text']  # List of training texts\n",
    "train_labels_33 : list[int] = train_dataset['label']  # Corresponding labels for training texts\n",
    "valid_texts_33 : list[str]= validation_dataset['text']  # List of validation texts\n",
    "valid_labels_33 : list[int] = validation_dataset['label']  # Corresponding labels for validation texts\n",
    "vocab_33 : set = vocab\n",
    "embedding_matrix_33 : dict[ str , np.ndarray]= np.load(\"embedding_matrix.npy\",allow_pickle='TRUE').item()\n",
    "embedding_matrix_values = np.array(list(embedding_matrix_33.values()), dtype=np.float32)\n",
    "embedding_matrix_tensor = torch.tensor(embedding_matrix_values, dtype=torch.float32)\n",
    "embedding_matrix_tensor = torch.tensor(embedding_matrix_values, dtype=torch.float32)\n",
    "\n",
    "train_dataset_33 : SentimentDataset_33 = SentimentDataset_33(train_texts_33, train_labels_33, vocab_33, embedding_matrix_33)\n",
    "valid_dataset_33 : SentimentDataset_33 = SentimentDataset_33(valid_texts_33, valid_labels_33, vocab_33, embedding_matrix_33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_33(model, iterator):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in iterator:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch).squeeze(1)\n",
    "        loss = criterion(output, y_batch.float())\n",
    "        loss.backward()\n",
    "        # for param in model.parameters():\n",
    "        #     if param.grad is not None:  # Ensure the gradient is not None\n",
    "        #         print(f\"Gradient norm for {param.shape}: {param.grad.data.norm()}\")\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_33(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in iterator:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move to GPU\n",
    "            output = model(X_batch).squeeze(1)\n",
    "            probs = model.sigmoid(output)\n",
    "            prediction = (probs >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(output, y_batch.float())\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(prediction.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy, epoch_loss / len(iterator)\n",
    "\n",
    "def train_and_validate_33(num_epochs, model, train_iterator, valid_iterator):\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_33(model, train_iterator)\n",
    "        accuracy , valid_loss = evaluate_33(model, valid_iterator)\n",
    "        print(f'Epoch {epoch + 1}: Train Loss = {train_loss:.3f}, Accuracy = {accuracy:.3f}, Val Loss = {valid_loss:.3f}')\n",
    "\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Check for convergence\n",
    "        if epochs_without_improvement >= 10:  # Convergence condition (no improvement for 5 epochs)\n",
    "            print(\"Convergence reached, stopping training.\")\n",
    "            break\n",
    "        \n",
    "    return best_val_acc, epochs_without_improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionBiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout_rate):\n",
    "        super(AttentionBiLSTM, self).__init__()\n",
    "        # Load pre-trained embeddings with trainable parameter\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size(1), hidden_dim, num_layers=1, \n",
    "                            bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        # Fully connected layer and dropout\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)  # LSTM output for all time steps\n",
    "        \n",
    "        # Attention weights\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        # Weighted sum of LSTM outputs\n",
    "        weighted_output = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Dropout and fully connected layer\n",
    "        out = self.dropout(weighted_output)\n",
    "        return self.fc(out)  # For classification, logits are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 32, Dropout Rate: 0.3, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.291, Accuracy = 0.500, Val Loss = 2.564\n",
      "Epoch 2: Train Loss = 0.328, Accuracy = 0.524, Val Loss = 1.746\n",
      "Epoch 3: Train Loss = 0.057, Accuracy = 0.658, Val Loss = 1.359\n",
      "Epoch 4: Train Loss = 0.009, Accuracy = 0.692, Val Loss = 1.299\n",
      "Epoch 5: Train Loss = 0.003, Accuracy = 0.714, Val Loss = 1.364\n",
      "Epoch 6: Train Loss = 0.002, Accuracy = 0.714, Val Loss = 1.403\n",
      "Epoch 7: Train Loss = 0.001, Accuracy = 0.712, Val Loss = 1.469\n",
      "Epoch 8: Train Loss = 0.001, Accuracy = 0.708, Val Loss = 1.528\n",
      "Epoch 9: Train Loss = 0.001, Accuracy = 0.707, Val Loss = 1.568\n",
      "Epoch 10: Train Loss = 0.000, Accuracy = 0.709, Val Loss = 1.607\n",
      "Epoch 11: Train Loss = 0.000, Accuracy = 0.705, Val Loss = 1.649\n",
      "Epoch 12: Train Loss = 0.000, Accuracy = 0.705, Val Loss = 1.687\n",
      "Epoch 13: Train Loss = 0.000, Accuracy = 0.707, Val Loss = 1.723\n",
      "Epoch 14: Train Loss = 0.000, Accuracy = 0.707, Val Loss = 1.759\n",
      "Epoch 15: Train Loss = 0.000, Accuracy = 0.707, Val Loss = 1.793\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 32, Validation Accuracy: 0.7138836772983115\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 32, Dropout Rate: 0.5, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.294, Accuracy = 0.500, Val Loss = 2.496\n",
      "Epoch 2: Train Loss = 0.271, Accuracy = 0.586, Val Loss = 1.423\n",
      "Epoch 3: Train Loss = 0.031, Accuracy = 0.692, Val Loss = 1.246\n",
      "Epoch 4: Train Loss = 0.006, Accuracy = 0.717, Val Loss = 1.284\n",
      "Epoch 5: Train Loss = 0.003, Accuracy = 0.712, Val Loss = 1.371\n",
      "Epoch 6: Train Loss = 0.002, Accuracy = 0.716, Val Loss = 1.463\n",
      "Epoch 7: Train Loss = 0.001, Accuracy = 0.714, Val Loss = 1.497\n",
      "Epoch 8: Train Loss = 0.001, Accuracy = 0.723, Val Loss = 1.582\n",
      "Epoch 9: Train Loss = 0.001, Accuracy = 0.719, Val Loss = 1.608\n",
      "Epoch 10: Train Loss = 0.000, Accuracy = 0.717, Val Loss = 1.646\n",
      "Epoch 11: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.686\n",
      "Epoch 12: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.724\n",
      "Epoch 13: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.759\n",
      "Epoch 14: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.793\n",
      "Epoch 15: Train Loss = 0.000, Accuracy = 0.720, Val Loss = 1.827\n",
      "Epoch 16: Train Loss = 0.000, Accuracy = 0.720, Val Loss = 1.861\n",
      "Epoch 17: Train Loss = 0.000, Accuracy = 0.721, Val Loss = 1.893\n",
      "Epoch 18: Train Loss = 0.000, Accuracy = 0.720, Val Loss = 1.925\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 32, Validation Accuracy: 0.723264540337711\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 64, Dropout Rate: 0.3, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.654, Accuracy = 0.500, Val Loss = 1.926\n",
      "Epoch 2: Train Loss = 0.435, Accuracy = 0.582, Val Loss = 1.239\n",
      "Epoch 3: Train Loss = 0.084, Accuracy = 0.692, Val Loss = 1.010\n",
      "Epoch 4: Train Loss = 0.016, Accuracy = 0.712, Val Loss = 1.025\n",
      "Epoch 5: Train Loss = 0.007, Accuracy = 0.718, Val Loss = 1.098\n",
      "Epoch 6: Train Loss = 0.004, Accuracy = 0.721, Val Loss = 1.160\n",
      "Epoch 7: Train Loss = 0.002, Accuracy = 0.721, Val Loss = 1.209\n",
      "Epoch 8: Train Loss = 0.002, Accuracy = 0.723, Val Loss = 1.256\n",
      "Epoch 9: Train Loss = 0.001, Accuracy = 0.723, Val Loss = 1.297\n",
      "Epoch 10: Train Loss = 0.001, Accuracy = 0.721, Val Loss = 1.331\n",
      "Epoch 11: Train Loss = 0.001, Accuracy = 0.723, Val Loss = 1.364\n",
      "Epoch 12: Train Loss = 0.001, Accuracy = 0.721, Val Loss = 1.394\n",
      "Epoch 13: Train Loss = 0.000, Accuracy = 0.722, Val Loss = 1.422\n",
      "Epoch 14: Train Loss = 0.000, Accuracy = 0.721, Val Loss = 1.449\n",
      "Epoch 15: Train Loss = 0.000, Accuracy = 0.721, Val Loss = 1.474\n",
      "Epoch 16: Train Loss = 0.000, Accuracy = 0.720, Val Loss = 1.499\n",
      "Epoch 17: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.521\n",
      "Epoch 18: Train Loss = 0.000, Accuracy = 0.720, Val Loss = 1.544\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 64, Validation Accuracy: 0.723264540337711\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.001, Batch Size: 64, Dropout Rate: 0.5, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.637, Accuracy = 0.500, Val Loss = 1.793\n",
      "Epoch 2: Train Loss = 0.417, Accuracy = 0.532, Val Loss = 1.408\n",
      "Epoch 3: Train Loss = 0.120, Accuracy = 0.673, Val Loss = 1.057\n",
      "Epoch 4: Train Loss = 0.022, Accuracy = 0.707, Val Loss = 1.021\n",
      "Epoch 5: Train Loss = 0.008, Accuracy = 0.720, Val Loss = 1.076\n",
      "Epoch 6: Train Loss = 0.004, Accuracy = 0.725, Val Loss = 1.149\n",
      "Epoch 7: Train Loss = 0.002, Accuracy = 0.720, Val Loss = 1.202\n",
      "Epoch 8: Train Loss = 0.002, Accuracy = 0.720, Val Loss = 1.250\n",
      "Epoch 9: Train Loss = 0.001, Accuracy = 0.720, Val Loss = 1.293\n",
      "Epoch 10: Train Loss = 0.001, Accuracy = 0.719, Val Loss = 1.330\n",
      "Epoch 11: Train Loss = 0.001, Accuracy = 0.719, Val Loss = 1.363\n",
      "Epoch 12: Train Loss = 0.001, Accuracy = 0.718, Val Loss = 1.394\n",
      "Epoch 13: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.424\n",
      "Epoch 14: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.451\n",
      "Epoch 15: Train Loss = 0.000, Accuracy = 0.720, Val Loss = 1.477\n",
      "Epoch 16: Train Loss = 0.000, Accuracy = 0.719, Val Loss = 1.502\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.001, Batch Size: 64, Validation Accuracy: 0.725140712945591\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 32, Dropout Rate: 0.3, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.256, Accuracy = 0.500, Val Loss = 8.110\n",
      "Epoch 2: Train Loss = 0.362, Accuracy = 0.500, Val Loss = 5.291\n",
      "Epoch 3: Train Loss = 0.341, Accuracy = 0.500, Val Loss = 7.017\n",
      "Epoch 4: Train Loss = 0.352, Accuracy = 0.500, Val Loss = 5.355\n",
      "Epoch 5: Train Loss = 0.427, Accuracy = 0.500, Val Loss = 6.317\n",
      "Epoch 6: Train Loss = 0.393, Accuracy = 0.501, Val Loss = 6.597\n",
      "Epoch 7: Train Loss = 0.377, Accuracy = 0.499, Val Loss = 6.116\n",
      "Epoch 8: Train Loss = 0.423, Accuracy = 0.505, Val Loss = 7.469\n",
      "Epoch 9: Train Loss = 0.498, Accuracy = 0.501, Val Loss = 4.557\n",
      "Epoch 10: Train Loss = 0.387, Accuracy = 0.498, Val Loss = 4.453\n",
      "Epoch 11: Train Loss = 0.413, Accuracy = 0.500, Val Loss = 3.105\n",
      "Epoch 12: Train Loss = 0.347, Accuracy = 0.499, Val Loss = 4.765\n",
      "Epoch 13: Train Loss = 0.372, Accuracy = 0.499, Val Loss = 6.428\n",
      "Epoch 14: Train Loss = 0.675, Accuracy = 0.500, Val Loss = 5.699\n",
      "Epoch 15: Train Loss = 0.459, Accuracy = 0.500, Val Loss = 8.009\n",
      "Epoch 16: Train Loss = 0.568, Accuracy = 0.501, Val Loss = 3.889\n",
      "Epoch 17: Train Loss = 0.421, Accuracy = 0.505, Val Loss = 3.132\n",
      "Epoch 18: Train Loss = 0.390, Accuracy = 0.501, Val Loss = 3.329\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 32, Validation Accuracy: 0.5046904315196998\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 32, Dropout Rate: 0.5, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.154, Accuracy = 0.500, Val Loss = 7.314\n",
      "Epoch 2: Train Loss = 0.258, Accuracy = 0.500, Val Loss = 5.897\n",
      "Epoch 3: Train Loss = 0.341, Accuracy = 0.500, Val Loss = 6.863\n",
      "Epoch 4: Train Loss = 0.449, Accuracy = 0.500, Val Loss = 7.227\n",
      "Epoch 5: Train Loss = 0.459, Accuracy = 0.499, Val Loss = 7.778\n",
      "Epoch 6: Train Loss = 0.469, Accuracy = 0.501, Val Loss = 6.253\n",
      "Epoch 7: Train Loss = 0.504, Accuracy = 0.500, Val Loss = 6.160\n",
      "Epoch 8: Train Loss = 0.569, Accuracy = 0.500, Val Loss = 5.899\n",
      "Epoch 9: Train Loss = 0.572, Accuracy = 0.500, Val Loss = 5.430\n",
      "Epoch 10: Train Loss = 0.580, Accuracy = 0.500, Val Loss = 4.098\n",
      "Epoch 11: Train Loss = 0.523, Accuracy = 0.500, Val Loss = 2.728\n",
      "Epoch 12: Train Loss = 0.367, Accuracy = 0.504, Val Loss = 6.765\n",
      "Epoch 13: Train Loss = 0.679, Accuracy = 0.500, Val Loss = 4.407\n",
      "Epoch 14: Train Loss = 0.520, Accuracy = 0.502, Val Loss = 3.332\n",
      "Epoch 15: Train Loss = 0.477, Accuracy = 0.500, Val Loss = 2.513\n",
      "Epoch 16: Train Loss = 0.465, Accuracy = 0.501, Val Loss = 2.448\n",
      "Epoch 17: Train Loss = 0.398, Accuracy = 0.500, Val Loss = 4.002\n",
      "Epoch 18: Train Loss = 0.389, Accuracy = 0.500, Val Loss = 5.760\n",
      "Epoch 19: Train Loss = 0.530, Accuracy = 0.504, Val Loss = 5.534\n",
      "Epoch 20: Train Loss = 0.563, Accuracy = 0.502, Val Loss = 4.348\n",
      "Epoch 21: Train Loss = 0.512, Accuracy = 0.496, Val Loss = 3.912\n",
      "Epoch 22: Train Loss = 0.549, Accuracy = 0.500, Val Loss = 3.115\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 32, Validation Accuracy: 0.5037523452157598\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 64, Dropout Rate: 0.3, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.347, Accuracy = 0.500, Val Loss = 5.655\n",
      "Epoch 2: Train Loss = 0.420, Accuracy = 0.500, Val Loss = 5.208\n",
      "Epoch 3: Train Loss = 0.622, Accuracy = 0.500, Val Loss = 5.221\n",
      "Epoch 4: Train Loss = 0.660, Accuracy = 0.500, Val Loss = 5.907\n",
      "Epoch 5: Train Loss = 0.738, Accuracy = 0.500, Val Loss = 7.553\n",
      "Epoch 6: Train Loss = 0.862, Accuracy = 0.505, Val Loss = 4.670\n",
      "Epoch 7: Train Loss = 0.773, Accuracy = 0.501, Val Loss = 4.063\n",
      "Epoch 8: Train Loss = 0.724, Accuracy = 0.502, Val Loss = 4.564\n",
      "Epoch 9: Train Loss = 0.809, Accuracy = 0.503, Val Loss = 4.198\n",
      "Epoch 10: Train Loss = 0.854, Accuracy = 0.503, Val Loss = 3.593\n",
      "Epoch 11: Train Loss = 0.704, Accuracy = 0.500, Val Loss = 3.421\n",
      "Epoch 12: Train Loss = 0.740, Accuracy = 0.498, Val Loss = 2.530\n",
      "Epoch 13: Train Loss = 0.705, Accuracy = 0.503, Val Loss = 2.217\n",
      "Epoch 14: Train Loss = 0.718, Accuracy = 0.501, Val Loss = 1.817\n",
      "Epoch 15: Train Loss = 0.680, Accuracy = 0.502, Val Loss = 1.608\n",
      "Epoch 16: Train Loss = 0.656, Accuracy = 0.518, Val Loss = 1.976\n",
      "Epoch 17: Train Loss = 0.561, Accuracy = 0.512, Val Loss = 4.607\n",
      "Epoch 18: Train Loss = 0.986, Accuracy = 0.499, Val Loss = 2.333\n",
      "Epoch 19: Train Loss = 0.658, Accuracy = 0.501, Val Loss = 1.891\n",
      "Epoch 20: Train Loss = 0.633, Accuracy = 0.501, Val Loss = 2.049\n",
      "Epoch 21: Train Loss = 0.599, Accuracy = 0.501, Val Loss = 2.832\n",
      "Epoch 22: Train Loss = 0.657, Accuracy = 0.499, Val Loss = 1.668\n",
      "Epoch 23: Train Loss = 0.638, Accuracy = 0.517, Val Loss = 1.349\n",
      "Epoch 24: Train Loss = 0.621, Accuracy = 0.525, Val Loss = 1.217\n",
      "Epoch 25: Train Loss = 0.411, Accuracy = 0.498, Val Loss = 8.074\n",
      "Epoch 26: Train Loss = 1.373, Accuracy = 0.500, Val Loss = 4.003\n",
      "Epoch 27: Train Loss = 1.001, Accuracy = 0.502, Val Loss = 1.711\n",
      "Epoch 28: Train Loss = 0.661, Accuracy = 0.504, Val Loss = 1.703\n",
      "Epoch 29: Train Loss = 0.626, Accuracy = 0.539, Val Loss = 1.235\n",
      "Epoch 30: Train Loss = 0.558, Accuracy = 0.546, Val Loss = 1.048\n",
      "Learning Rate: 0.01, Batch Size: 64, Validation Accuracy: 0.5459662288930581\n",
      "Training with the following hyperparameters:\n",
      "Learning Rate: 0.01, Batch Size: 64, Dropout Rate: 0.5, Hidden Size: 128\n",
      "Epoch 1: Train Loss = 0.325, Accuracy = 0.500, Val Loss = 5.359\n",
      "Epoch 2: Train Loss = 0.421, Accuracy = 0.500, Val Loss = 5.302\n",
      "Epoch 3: Train Loss = 0.700, Accuracy = 0.501, Val Loss = 4.800\n",
      "Epoch 4: Train Loss = 0.739, Accuracy = 0.500, Val Loss = 4.654\n",
      "Epoch 5: Train Loss = 0.571, Accuracy = 0.500, Val Loss = 6.334\n",
      "Epoch 6: Train Loss = 0.894, Accuracy = 0.500, Val Loss = 7.372\n",
      "Epoch 7: Train Loss = 1.031, Accuracy = 0.500, Val Loss = 3.993\n",
      "Epoch 8: Train Loss = 0.949, Accuracy = 0.503, Val Loss = 4.244\n",
      "Epoch 9: Train Loss = 0.815, Accuracy = 0.515, Val Loss = 5.054\n",
      "Epoch 10: Train Loss = 0.906, Accuracy = 0.500, Val Loss = 3.248\n",
      "Epoch 11: Train Loss = 0.693, Accuracy = 0.501, Val Loss = 3.135\n",
      "Epoch 12: Train Loss = 0.746, Accuracy = 0.500, Val Loss = 2.204\n",
      "Epoch 13: Train Loss = 0.692, Accuracy = 0.501, Val Loss = 2.382\n",
      "Epoch 14: Train Loss = 0.582, Accuracy = 0.501, Val Loss = 4.498\n",
      "Epoch 15: Train Loss = 0.926, Accuracy = 0.503, Val Loss = 2.653\n",
      "Epoch 16: Train Loss = 0.674, Accuracy = 0.507, Val Loss = 4.480\n",
      "Epoch 17: Train Loss = 0.858, Accuracy = 0.506, Val Loss = 3.222\n",
      "Epoch 18: Train Loss = 0.792, Accuracy = 0.505, Val Loss = 1.931\n",
      "Epoch 19: Train Loss = 0.673, Accuracy = 0.505, Val Loss = 2.078\n",
      "Convergence reached, stopping training.\n",
      "Learning Rate: 0.01, Batch Size: 64, Validation Accuracy: 0.5150093808630394\n",
      "Best Model Configuration: {'learning_rate': 0.001, 'batch_size': 64} with Validation Accuracy: 0.725140712945591 over 10 epochs\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01]\n",
    "batch_sizes = [32, 64]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "hidden_size = 128\n",
    "output_dim = 1\n",
    "\n",
    "best_val_acc = 0\n",
    "best_hyperparams = {}\n",
    "#switch to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for dr in dropout_rates:\n",
    "            print(\"Training with the following hyperparameters:\")\n",
    "            print(f\"Learning Rate: {lr}, Batch Size: {bs}, Dropout Rate: {dr}, Hidden Size: {hidden_size}\")\n",
    "            model = AttentionBiLSTM(embedding_matrix_tensor, 128, output_dim, dr).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            train_iterator_33 = DataLoader(train_dataset_33, bs)\n",
    "            valid_iterator_33 = DataLoader(valid_dataset_33, bs)\n",
    "                \n",
    "            # Train and validate\n",
    "            val_acc, epochs_used = train_and_validate_33(30, model, train_iterator_33, valid_iterator_33)\n",
    "            print(f\"Learning Rate: {lr}, Batch Size: {bs}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "            # Update best parameters\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_hyperparams = {\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': bs\n",
    "                }\n",
    "                best_epochs = epochs_used\n",
    "\n",
    "# Print the best configuration\n",
    "print(f\"Best Model Configuration: {best_hyperparams} with Validation Accuracy: {best_val_acc} over {best_epochs} epochs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
